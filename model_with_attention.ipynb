{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Razer\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, History\n",
    "from keras.losses import *\n",
    "from keras.utils import Sequence\n",
    "from data_utilities import DataGenerator\n",
    "from keras import backend as K\n",
    "import os, time\n",
    "from matplotlib import pyplot as plt\n",
    "from model_utilities import DistortImages, VisualizeImage, VisualizeImageWithPrediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our Attention Model Here\n",
    "\n",
    "Here, We will first define the attention block, then we will use it in a keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AttentionBlock(x, shortcut, num_filters):\n",
    "    '''\n",
    "    function that defines the attention gated block for our U-Net Model\n",
    "\n",
    "    Arguments:\n",
    "        x {tensor} -- Input from previous decoding upscale layer\n",
    "        shortcut {tensor} -- Corresponding input from the same level encoder layer\n",
    "        num_filters {int} -- total filters used for that layer in decoder\n",
    "\n",
    "    Returns:\n",
    "        {tensor} -- the output of the attention gate to be concatenated with that layer in decoder\n",
    "    '''\n",
    "    g1 = Conv2D(num_filters, kernel_size = 1, kernel_initializer = \"he_normal\", padding = \"same\")(shortcut)\n",
    "    x1 = Conv2D(num_filters, kernel_size = 1, kernel_initializer = \"he_normal\", padding = \"same\")(x)\n",
    "    \n",
    "    g1_x1 = Add()([g1, x1])\n",
    "    psi = Activation(\"relu\")(g1_x1)\n",
    "    psi = Conv2D(1, kernel_size = 1, padding = \"same\")(psi)\n",
    "    psi = Activation(\"sigmoid\")(psi)\n",
    "    x = Multiply()([x, psi])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNet(shape, reuse = False, n_out = 1):\n",
    "    \n",
    "    nnx = int(shape[1])\n",
    "    nny = int(shape[2])\n",
    "    nnz = int(shape[3])\n",
    "    print(\" * Input: size of image: %d %d %d\" % (nnx, nny, nnz))\n",
    "    with tf.variable_scope(\"u_net\", reuse = reuse):\n",
    "        \n",
    "        # Encoder\n",
    "        inputs = Input((nnx, nny, nnz))\n",
    "        \n",
    "        conv1 = Conv2D(64, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(inputs)\n",
    "        conv1 = Conv2D(64, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(conv1)\n",
    "        pool1 = MaxPooling2D(pool_size = (2, 2))(conv1)\n",
    "        \n",
    "        conv2 = Conv2D(128, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(pool1)\n",
    "        conv2 = Conv2D(128, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(conv2)\n",
    "        pool2 = MaxPool2D(pool_size = (2, 2))(conv2)\n",
    "        \n",
    "        conv3 = Conv2D(256, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(pool2)\n",
    "        conv3 = Conv2D(256, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(conv3)\n",
    "        pool3 = MaxPooling2D(pool_size = (2, 2))(conv3)\n",
    "        \n",
    "        conv4 = Conv2D(512, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(pool3)\n",
    "        conv4 = Conv2D(512, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(conv4)\n",
    "        conv4 = Dropout(0.5)(conv4)\n",
    "        pool4 = MaxPooling2D(pool_size = (2, 2))(conv4)\n",
    "        \n",
    "        conv5 = Conv2D(1024, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(pool4)\n",
    "        conv5 = Conv2D(1024, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(conv5)\n",
    "        conv5 = Dropout(0.5)(conv5)\n",
    "        \n",
    "        # Decoder ;_;\n",
    "        up6 = Deconv2D(512, 3, strides = (2, 2), padding = \"same\")(conv5)\n",
    "        merge6 = concatenate([conv4, up6], axis = -1)\n",
    "        conv6 = Conv2D(512, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(merge6)\n",
    "        conv6 = Conv2D(512, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(conv6)\n",
    "        \n",
    "        up7 = Deconv2D(256, 3, strides = (2, 2), padding = \"same\")(conv6)\n",
    "        merge7 = concatenate([conv3, up7], axis = -1)\n",
    "        conv7 = Conv2D(256, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(merge7)\n",
    "        conv7 = Conv2D(256, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(conv7)\n",
    "        \n",
    "        up8 = Deconv2D(128, 3, strides = (2, 2), padding = \"same\")(conv7)\n",
    "        merge8 = concatenate([conv2, up8], axis = -1)\n",
    "        conv8 = Conv2D(128, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(merge8)\n",
    "        conv8 = Conv2D(128, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(conv8)\n",
    "        \n",
    "        up9 = Deconv2D(64, 3, strides = (2, 2), padding = \"same\")(conv8)\n",
    "        merge9 = concatenate([conv1, up9], axis = -1)\n",
    "        conv9 = Conv2D(64, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(merge9)\n",
    "        conv9 = Conv2D(64, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(conv9)\n",
    "        conv9 = Conv2D(2, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(conv9)\n",
    "\n",
    "        conv10 = Conv2D(n_out, 1, activation = \"sigmoid\", padding = \"same\", kernel_initializer = \"he_normal\")(conv9)\n",
    "        \n",
    "        model = Model(inputs = inputs, outputs = conv10)\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNetWithAttention(shape, reuse = False, n_out = 1):\n",
    "    \n",
    "    nnx = int(shape[1])\n",
    "    nny = int(shape[2])\n",
    "    nnz = int(shape[3])\n",
    "    print(\" * Input: size of image: %d %d %d\" % (nnx, nny, nnz))\n",
    "    with tf.variable_scope(\"u_net\", reuse = reuse):\n",
    "        \n",
    "        # Encoder\n",
    "        inputs = Input((nnx, nny, nnz))\n",
    "        \n",
    "        conv1 = Conv2D(64, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(inputs)\n",
    "        conv1 = Conv2D(64, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(conv1)\n",
    "        pool1 = MaxPooling2D(pool_size = (2, 2))(conv1)\n",
    "        \n",
    "        conv2 = Conv2D(128, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(pool1)\n",
    "        conv2 = Conv2D(128, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(conv2)\n",
    "        pool2 = MaxPool2D(pool_size = (2, 2))(conv2)\n",
    "        \n",
    "        conv3 = Conv2D(256, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(pool2)\n",
    "        conv3 = Conv2D(256, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(conv3)\n",
    "        pool3 = MaxPooling2D(pool_size = (2, 2))(conv3)\n",
    "        \n",
    "        conv4 = Conv2D(512, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(pool3)\n",
    "        conv4 = Conv2D(512, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(conv4)\n",
    "        conv4 = Dropout(0.5)(conv4)\n",
    "        pool4 = MaxPooling2D(pool_size = (2, 2))(conv4)\n",
    "        \n",
    "        conv5 = Conv2D(1024, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(pool4)\n",
    "        conv5 = Conv2D(1024, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(conv5)\n",
    "        conv5 = Dropout(0.5)(conv5)\n",
    "        \n",
    "        # Decoder ;_;\n",
    "        up6 = Deconv2D(512, 3, strides = (2, 2), padding = \"same\")(conv5)\n",
    "        attn6 = AttentionBlock(up6, conv4, 512)\n",
    "        merge6 = concatenate([attn6, up6], axis = -1)\n",
    "        conv6 = Conv2D(512, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(merge6)\n",
    "        conv6 = Conv2D(512, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(conv6)\n",
    "        \n",
    "        up7 = Deconv2D(256, 3, strides = (2, 2), padding = \"same\")(conv6)\n",
    "        attn7 = AttentionBlock(up7, conv3, 256)\n",
    "        merge7 = concatenate([attn7, up7], axis = -1)\n",
    "        conv7 = Conv2D(256, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(merge7)\n",
    "        conv7 = Conv2D(256, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(conv7)\n",
    "        \n",
    "        up8 = Deconv2D(128, 3, strides = (2, 2), padding = \"same\")(conv7)\n",
    "        attn8 = AttentionBlock(up8, conv2, 128)\n",
    "        merge8 = concatenate([attn8, up8], axis = -1)\n",
    "        conv8 = Conv2D(128, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(merge8)\n",
    "        conv8 = Conv2D(128, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(conv8)\n",
    "        \n",
    "        up9 = Deconv2D(64, 3, strides = (2, 2), padding = \"same\")(conv8)\n",
    "        attn9 = AttentionBlock(up9, conv1, 64)\n",
    "        merge9 = concatenate([attn9, up9], axis = -1)\n",
    "        conv9 = Conv2D(64, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(merge9)\n",
    "        conv9 = Conv2D(64, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(conv9)\n",
    "        conv9 = Conv2D(2, 3, activation = \"relu\", padding = \"same\", kernel_initializer = \"he_normal\")(conv9)\n",
    "\n",
    "        conv10 = Conv2D(n_out, 1, activation = \"sigmoid\", padding = \"same\", kernel_initializer = \"he_normal\")(conv9)\n",
    "        \n",
    "        model = Model(inputs = inputs, outputs = conv10)\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up folders for checkpoints and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "save_dir = \"./checkpoint\"\n",
    "task = \"all\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "if not os.path.exists(\"./sample/{}\".format(task)):\n",
    "    os.mkdir(\"./sample\")\n",
    "    os.mkdir(\"./sample/{}\".format(task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./test/{}\".format(task)):\n",
    "    os.mkdir(\"./test\")\n",
    "    os.mkdir(\"./test/{}\".format(task))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and load data\n",
    "Here, we will load the entire data and preprocess it as and when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "13\n",
      "28\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "(1, 240, 240, 153)\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "(1, 240, 240, 153)\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "(1, 240, 240, 153)\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "float32\n",
      "(1, 240, 240, 153)\n",
      "{'DWI': {'mean': 21.078682, 'std': 44.95277}, 'Flair': {'mean': 24.302284, 'std': 50.49608}, 'T1': {'mean': 31.60154, 'std': 63.932735}, 'T2': {'mean': 44.153976, 'std': 92.98761}}\n",
      " HGG Validation\n",
      "finished 22\n",
      "finished 23\n",
      "finished 24\n",
      "finished 25\n",
      "finished 26\n",
      "finished 27\n",
      "finished 28\n",
      "(1071, 240, 240, 4)\n",
      "(1071, 240, 240)\n",
      " HGG Train\n",
      "finished 1\n",
      "finished 2\n",
      "finished 3\n",
      "finished 4\n",
      "finished 5\n",
      "finished 6\n",
      "finished 7\n",
      "finished 8\n",
      "finished 9\n",
      "finished 10\n",
      "finished 11\n",
      "finished 12\n",
      "finished 13\n",
      "finished 14\n",
      "finished 15\n",
      "finished 16\n",
      "finished 17\n",
      "finished 18\n",
      "finished 19\n",
      "finished 20\n",
      "(3060, 240, 240, 4)\n",
      "(3060, 240, 240)\n"
     ]
    }
   ],
   "source": [
    "import prepare_data_with_valid4 as dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.49429357051849365, 29.490262985229492]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "XTrain = dataset.X_train_input\n",
    "yTrain = dataset.X_train_target[:, :, :, np.newaxis]\n",
    "XTest = dataset.X_dev_input\n",
    "yTest = dataset.X_dev_target[:, :, :, np.newaxis]\n",
    "\n",
    "yTrain = (yTrain > 0).astype(int)\n",
    "yTest = (yTest > 0).astype(int)\n",
    "\n",
    "# Visualize a slice\n",
    "X = np.asarray(XTrain[80])\n",
    "y = np.asarray(yTrain[80])\n",
    "nw, nh, nz = X.shape\n",
    "VisualizeImage(X, y, 'sample/{}/_train_im.png'.format(task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3060, 240, 240, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTest_ar = np.asarray(yTest)\n",
    "yTest_ar = np.squeeze(yTest)\n",
    "yTest_ar.shape\n",
    "y_sq = np.squeeze(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x160829553c8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(y_sq, cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x160829e04e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(yTest_ar[48, :, :], cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation Testing\n",
    "\n",
    "Here, we will check the preprocessing functions and make sure that they work properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.4942935705184938, 28.5815349355855]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.49429357051849365, 29.490262985229492]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 240, 1)\n",
      "(240, 240, 1)\n",
      "(240, 240, 1)\n",
      "(240, 240, 1)\n",
      "(240, 240, 1)\n",
      "(240, 240, 4)\n",
      "-0.4942935705184938\n",
      "28.5815349355855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.4942935705184939, 28.632615247297224]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.49429357051849365, 29.490262985229492]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.4942935705184939, 28.564338152223876]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.49429357051849365, 29.490262985229492]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.4942935705184939, 28.59779287524779]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.49429357051849365, 29.490262985229492]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.4942935705184939, 28.492731624595635]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.49429357051849365, 29.490262985229492]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.4942935705184939, 28.75805324039105]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.49429357051849365, 29.490262985229492]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.4942935705184939, 28.898815354251607]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.49429357051849365, 29.490262985229492]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.4942935705184938, 28.665755490414718]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.49429357051849365, 29.490262985229492]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.4942935705184938, 28.702295406601163]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.49429357051849365, 29.490262985229492]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.4942935705184939, 28.755386874718287]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "WARNING:root:Lossy conversion from float64 to uint8. Range [-0.49429357051849365, 29.490262985229492]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "# Show the data augmentation\n",
    "for i in range(10):\n",
    "    x_flair, x_t1, x_t1ce, x_t2, label = DistortImages([X[:, :, 0, np.newaxis], \n",
    "                                                   X[:, :, 1, np.newaxis], \n",
    "                                                   X[:, :, 2, np.newaxis], \n",
    "                                                   X[:, :, 3, np.newaxis], y])\n",
    "    X_dis = np.concatenate((x_flair, x_t1, x_t1ce, x_t2), axis = 2)\n",
    "    X_d = np.concatenate((X[:, :, 0, np.newaxis], X[:, :, 1, np.newaxis], X[:, :, 2, np.newaxis], X[:, :, 3, np.newaxis]), axis = 2)\n",
    "    VisualizeImage(X_dis, label, 'sample/{}/_train_im_aug{}.png'.format(task, i))\n",
    "    VisualizeImage(X_d, label, 'sample/{}/_sample_im_aug{}.png'.format(task, i))\n",
    "    if i == 0:\n",
    "        print(x_flair.shape, x_t1.shape, x_t1ce.shape, x_t2.shape, label.shape, sep=\"\\n\")\n",
    "        print(X_dis.shape, X_dis.min(), X_dis.max(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameter Setting\n",
    "Here we will set the hyperparams necessary to jolt our model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "lr = 0.0001\n",
    "beta1 = 0.9\n",
    "n_epoch = 5\n",
    "printFreq = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our Loss Functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth = 1):\n",
    "    '''\n",
    "    Function that defines the dice_coef\n",
    "    Dice = (2*|X & Y|)/ (|X|+ |Y|) = 2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "    '''\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis = -1)\n",
    "    return (2. * intersection + smooth)/(K.sum(K.square(y_true), -1) + K.sum(K.square(y_pred), -1) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Input: size of image: 240 240 4\n"
     ]
    }
   ],
   "source": [
    "model = UNet(shape = (batch_size, nw, nh, nz))\n",
    "model.compile(optimizer = Adam(lr=lr, beta_1=beta1), loss = dice_coef_loss, metrics = [dice_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 240, 240, 4)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 240, 240, 64) 2368        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 240, 240, 64) 36928       conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 120, 120, 64) 0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 120, 120, 128 73856       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 120, 120, 128 147584      conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 60, 60, 128)  0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 60, 60, 256)  295168      max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 60, 60, 256)  590080      conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 30, 30, 256)  0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 30, 30, 512)  1180160     max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 30, 30, 512)  2359808     conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 30, 30, 512)  0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 15, 15, 512)  0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 15, 15, 1024) 4719616     max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 15, 15, 1024) 9438208     conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 15, 15, 1024) 0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 30, 30, 512)  4719104     dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 30, 30, 1024) 0           dropout_3[0][0]                  \n",
      "                                                                 conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 30, 30, 512)  4719104     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 30, 30, 512)  2359808     conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTrans (None, 60, 60, 256)  1179904     conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 60, 60, 512)  0           conv2d_26[0][0]                  \n",
      "                                                                 conv2d_transpose_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 60, 60, 256)  1179904     concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 60, 60, 256)  590080      conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTrans (None, 120, 120, 128 295040      conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 120, 120, 256 0           conv2d_24[0][0]                  \n",
      "                                                                 conv2d_transpose_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 120, 120, 128 295040      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 120, 120, 128 147584      conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTrans (None, 240, 240, 64) 73792       conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 240, 240, 128 0           conv2d_22[0][0]                  \n",
      "                                                                 conv2d_transpose_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 240, 240, 64) 73792       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 240, 240, 64) 36928       conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 240, 240, 2)  1154        conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 240, 240, 1)  3           conv2d_39[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 34,515,013\n",
      "Trainable params: 34,515,013\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGen = DataGenerator(XTrain, yTrain, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177/612 [=======>......................] - ETA: 2:07:45 - loss: 0.2072 - dice_coef: 0.79 - ETA: 1:06:12 - loss: 0.2010 - dice_coef: 0.79 - ETA: 45:41 - loss: 0.1953 - dice_coef: 0.8047 - ETA: 35:25 - loss: 0.1869 - dice_coef: 0.81 - ETA: 29:15 - loss: 0.1844 - dice_coef: 0.81 - ETA: 25:09 - loss: 0.1779 - dice_coef: 0.82 - ETA: 22:12 - loss: 0.1756 - dice_coef: 0.82 - ETA: 19:59 - loss: 0.1728 - dice_coef: 0.82 - ETA: 18:16 - loss: 0.1660 - dice_coef: 0.83 - ETA: 16:54 - loss: 0.1577 - dice_coef: 0.84 - ETA: 15:46 - loss: 0.1491 - dice_coef: 0.85 - ETA: 14:50 - loss: 0.1382 - dice_coef: 0.86 - ETA: 14:02 - loss: 0.1280 - dice_coef: 0.87 - ETA: 13:20 - loss: 0.1193 - dice_coef: 0.88 - ETA: 12:45 - loss: 0.1114 - dice_coef: 0.88 - ETA: 12:14 - loss: 0.1044 - dice_coef: 0.89 - ETA: 11:46 - loss: 0.0994 - dice_coef: 0.90 - ETA: 11:21 - loss: 0.0939 - dice_coef: 0.90 - ETA: 10:59 - loss: 0.0892 - dice_coef: 0.91 - ETA: 10:39 - loss: 0.0853 - dice_coef: 0.91 - ETA: 10:21 - loss: 0.0813 - dice_coef: 0.91 - ETA: 10:04 - loss: 0.0776 - dice_coef: 0.92 - ETA: 9:49 - loss: 0.0742 - dice_coef: 0.9258 - ETA: 9:35 - loss: 0.0711 - dice_coef: 0.928 - ETA: 9:22 - loss: 0.0683 - dice_coef: 0.931 - ETA: 9:10 - loss: 0.0656 - dice_coef: 0.934 - ETA: 8:59 - loss: 0.0637 - dice_coef: 0.936 - ETA: 8:49 - loss: 0.0614 - dice_coef: 0.938 - ETA: 8:39 - loss: 0.0594 - dice_coef: 0.940 - ETA: 8:30 - loss: 0.0574 - dice_coef: 0.942 - ETA: 8:22 - loss: 0.0555 - dice_coef: 0.944 - ETA: 8:14 - loss: 0.0538 - dice_coef: 0.946 - ETA: 8:07 - loss: 0.0522 - dice_coef: 0.947 - ETA: 7:59 - loss: 0.0513 - dice_coef: 0.948 - ETA: 7:53 - loss: 0.0498 - dice_coef: 0.950 - ETA: 7:46 - loss: 0.0485 - dice_coef: 0.951 - ETA: 7:40 - loss: 0.0474 - dice_coef: 0.952 - ETA: 7:34 - loss: 0.0462 - dice_coef: 0.953 - ETA: 7:29 - loss: 0.0450 - dice_coef: 0.955 - ETA: 7:24 - loss: 0.0439 - dice_coef: 0.956 - ETA: 7:19 - loss: 0.0428 - dice_coef: 0.957 - ETA: 7:14 - loss: 0.0422 - dice_coef: 0.957 - ETA: 7:09 - loss: 0.0412 - dice_coef: 0.958 - ETA: 7:05 - loss: 0.0403 - dice_coef: 0.959 - ETA: 7:00 - loss: 0.0394 - dice_coef: 0.960 - ETA: 6:56 - loss: 0.0391 - dice_coef: 0.960 - ETA: 6:52 - loss: 0.0383 - dice_coef: 0.961 - ETA: 6:49 - loss: 0.0375 - dice_coef: 0.962 - ETA: 6:45 - loss: 0.0368 - dice_coef: 0.963 - ETA: 6:41 - loss: 0.0360 - dice_coef: 0.964 - ETA: 6:38 - loss: 0.0353 - dice_coef: 0.964 - ETA: 6:35 - loss: 0.0348 - dice_coef: 0.965 - ETA: 6:32 - loss: 0.0345 - dice_coef: 0.965 - ETA: 6:28 - loss: 0.0338 - dice_coef: 0.966 - ETA: 6:26 - loss: 0.0332 - dice_coef: 0.966 - ETA: 6:23 - loss: 0.0328 - dice_coef: 0.967 - ETA: 6:20 - loss: 0.0322 - dice_coef: 0.967 - ETA: 6:17 - loss: 0.0318 - dice_coef: 0.968 - ETA: 6:14 - loss: 0.0312 - dice_coef: 0.968 - ETA: 6:12 - loss: 0.0309 - dice_coef: 0.969 - ETA: 6:09 - loss: 0.0309 - dice_coef: 0.969 - ETA: 6:07 - loss: 0.0308 - dice_coef: 0.969 - ETA: 6:05 - loss: 0.0305 - dice_coef: 0.969 - ETA: 6:02 - loss: 0.0303 - dice_coef: 0.969 - ETA: 6:00 - loss: 0.0299 - dice_coef: 0.970 - ETA: 5:58 - loss: 0.0296 - dice_coef: 0.970 - ETA: 5:56 - loss: 0.0292 - dice_coef: 0.970 - ETA: 5:54 - loss: 0.0291 - dice_coef: 0.970 - ETA: 5:52 - loss: 0.0289 - dice_coef: 0.971 - ETA: 5:50 - loss: 0.0287 - dice_coef: 0.971 - ETA: 5:48 - loss: 0.0283 - dice_coef: 0.971 - ETA: 5:46 - loss: 0.0285 - dice_coef: 0.971 - ETA: 5:44 - loss: 0.0281 - dice_coef: 0.971 - ETA: 5:42 - loss: 0.0277 - dice_coef: 0.972 - ETA: 5:40 - loss: 0.0276 - dice_coef: 0.972 - ETA: 5:38 - loss: 0.0273 - dice_coef: 0.972 - ETA: 5:37 - loss: 0.0269 - dice_coef: 0.973 - ETA: 5:35 - loss: 0.0267 - dice_coef: 0.973 - ETA: 5:33 - loss: 0.0264 - dice_coef: 0.973 - ETA: 5:32 - loss: 0.0261 - dice_coef: 0.973 - ETA: 5:30 - loss: 0.0258 - dice_coef: 0.974 - ETA: 5:28 - loss: 0.0255 - dice_coef: 0.974 - ETA: 5:27 - loss: 0.0252 - dice_coef: 0.974 - ETA: 5:25 - loss: 0.0249 - dice_coef: 0.975 - ETA: 5:24 - loss: 0.0246 - dice_coef: 0.975 - ETA: 5:22 - loss: 0.0243 - dice_coef: 0.975 - ETA: 5:21 - loss: 0.0241 - dice_coef: 0.975 - ETA: 5:19 - loss: 0.0239 - dice_coef: 0.976 - ETA: 5:18 - loss: 0.0238 - dice_coef: 0.976 - ETA: 5:17 - loss: 0.0239 - dice_coef: 0.976 - ETA: 5:15 - loss: 0.0238 - dice_coef: 0.976 - ETA: 5:14 - loss: 0.0236 - dice_coef: 0.976 - ETA: 5:13 - loss: 0.0235 - dice_coef: 0.976 - ETA: 5:11 - loss: 0.0233 - dice_coef: 0.976 - ETA: 5:10 - loss: 0.0230 - dice_coef: 0.977 - ETA: 5:09 - loss: 0.0228 - dice_coef: 0.977 - ETA: 5:07 - loss: 0.0225 - dice_coef: 0.977 - ETA: 5:06 - loss: 0.0226 - dice_coef: 0.977 - ETA: 5:05 - loss: 0.0224 - dice_coef: 0.977 - ETA: 5:04 - loss: 0.0221 - dice_coef: 0.977 - ETA: 5:03 - loss: 0.0219 - dice_coef: 0.978 - ETA: 5:01 - loss: 0.0217 - dice_coef: 0.978 - ETA: 5:00 - loss: 0.0215 - dice_coef: 0.978 - ETA: 4:59 - loss: 0.0214 - dice_coef: 0.978 - ETA: 4:58 - loss: 0.0212 - dice_coef: 0.978 - ETA: 4:57 - loss: 0.0210 - dice_coef: 0.979 - ETA: 4:56 - loss: 0.0208 - dice_coef: 0.979 - ETA: 4:54 - loss: 0.0207 - dice_coef: 0.979 - ETA: 4:53 - loss: 0.0205 - dice_coef: 0.979 - ETA: 4:52 - loss: 0.0203 - dice_coef: 0.979 - ETA: 4:51 - loss: 0.0204 - dice_coef: 0.979 - ETA: 4:50 - loss: 0.0202 - dice_coef: 0.979 - ETA: 4:49 - loss: 0.0201 - dice_coef: 0.979 - ETA: 4:48 - loss: 0.0199 - dice_coef: 0.980 - ETA: 4:47 - loss: 0.0197 - dice_coef: 0.980 - ETA: 4:46 - loss: 0.0195 - dice_coef: 0.980 - ETA: 4:45 - loss: 0.0194 - dice_coef: 0.980 - ETA: 4:44 - loss: 0.0192 - dice_coef: 0.980 - ETA: 4:43 - loss: 0.0191 - dice_coef: 0.980 - ETA: 4:42 - loss: 0.0190 - dice_coef: 0.981 - ETA: 4:41 - loss: 0.0189 - dice_coef: 0.981 - ETA: 4:40 - loss: 0.0187 - dice_coef: 0.981 - ETA: 4:39 - loss: 0.0186 - dice_coef: 0.981 - ETA: 4:38 - loss: 0.0185 - dice_coef: 0.981 - ETA: 4:37 - loss: 0.0183 - dice_coef: 0.981 - ETA: 4:36 - loss: 0.0182 - dice_coef: 0.981 - ETA: 4:35 - loss: 0.0182 - dice_coef: 0.981 - ETA: 4:34 - loss: 0.0181 - dice_coef: 0.981 - ETA: 4:33 - loss: 0.0180 - dice_coef: 0.982 - ETA: 4:33 - loss: 0.0178 - dice_coef: 0.982 - ETA: 4:32 - loss: 0.0177 - dice_coef: 0.982 - ETA: 4:31 - loss: 0.0176 - dice_coef: 0.982 - ETA: 4:30 - loss: 0.0175 - dice_coef: 0.982 - ETA: 4:29 - loss: 0.0174 - dice_coef: 0.982 - ETA: 4:28 - loss: 0.0173 - dice_coef: 0.982 - ETA: 4:27 - loss: 0.0173 - dice_coef: 0.982 - ETA: 4:26 - loss: 0.0172 - dice_coef: 0.982 - ETA: 4:26 - loss: 0.0171 - dice_coef: 0.982 - ETA: 4:25 - loss: 0.0170 - dice_coef: 0.983 - ETA: 4:24 - loss: 0.0169 - dice_coef: 0.983 - ETA: 4:23 - loss: 0.0167 - dice_coef: 0.983 - ETA: 4:22 - loss: 0.0166 - dice_coef: 0.983 - ETA: 4:21 - loss: 0.0165 - dice_coef: 0.983 - ETA: 4:20 - loss: 0.0165 - dice_coef: 0.983 - ETA: 4:20 - loss: 0.0164 - dice_coef: 0.983 - ETA: 4:19 - loss: 0.0163 - dice_coef: 0.983 - ETA: 4:18 - loss: 0.0162 - dice_coef: 0.983 - ETA: 4:17 - loss: 0.0161 - dice_coef: 0.983 - ETA: 4:16 - loss: 0.0162 - dice_coef: 0.983 - ETA: 4:16 - loss: 0.0161 - dice_coef: 0.983 - ETA: 4:15 - loss: 0.0160 - dice_coef: 0.984 - ETA: 4:14 - loss: 0.0160 - dice_coef: 0.984 - ETA: 4:13 - loss: 0.0159 - dice_coef: 0.984 - ETA: 4:12 - loss: 0.0158 - dice_coef: 0.984 - ETA: 4:12 - loss: 0.0157 - dice_coef: 0.984 - ETA: 4:11 - loss: 0.0157 - dice_coef: 0.984 - ETA: 4:10 - loss: 0.0156 - dice_coef: 0.984 - ETA: 4:09 - loss: 0.0156 - dice_coef: 0.984 - ETA: 4:09 - loss: 0.0155 - dice_coef: 0.984 - ETA: 4:08 - loss: 0.0154 - dice_coef: 0.984 - ETA: 4:07 - loss: 0.0153 - dice_coef: 0.984 - ETA: 4:06 - loss: 0.0153 - dice_coef: 0.984 - ETA: 4:05 - loss: 0.0152 - dice_coef: 0.984 - ETA: 4:05 - loss: 0.0151 - dice_coef: 0.984 - ETA: 4:04 - loss: 0.0150 - dice_coef: 0.985 - ETA: 4:03 - loss: 0.0149 - dice_coef: 0.985 - ETA: 4:02 - loss: 0.0148 - dice_coef: 0.985 - ETA: 4:02 - loss: 0.0148 - dice_coef: 0.985 - ETA: 4:01 - loss: 0.0147 - dice_coef: 0.985 - ETA: 4:00 - loss: 0.0146 - dice_coef: 0.985 - ETA: 4:00 - loss: 0.0145 - dice_coef: 0.985 - ETA: 3:59 - loss: 0.0145 - dice_coef: 0.985 - ETA: 3:58 - loss: 0.0144 - dice_coef: 0.985 - ETA: 3:57 - loss: 0.0143 - dice_coef: 0.985 - ETA: 3:57 - loss: 0.0142 - dice_coef: 0.985 - ETA: 3:56 - loss: 0.0142 - dice_coef: 0.985 - ETA: 3:55 - loss: 0.0141 - dice_coef: 0.9859\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/612 [================>.............] - ETA: 3:55 - loss: 0.0140 - dice_coef: 0.986 - ETA: 3:54 - loss: 0.0139 - dice_coef: 0.986 - ETA: 3:53 - loss: 0.0139 - dice_coef: 0.986 - ETA: 3:52 - loss: 0.0138 - dice_coef: 0.986 - ETA: 3:52 - loss: 0.0137 - dice_coef: 0.986 - ETA: 3:51 - loss: 0.0136 - dice_coef: 0.986 - ETA: 3:50 - loss: 0.0136 - dice_coef: 0.986 - ETA: 3:50 - loss: 0.0135 - dice_coef: 0.986 - ETA: 3:49 - loss: 0.0134 - dice_coef: 0.986 - ETA: 3:48 - loss: 0.0134 - dice_coef: 0.986 - ETA: 3:48 - loss: 0.0133 - dice_coef: 0.986 - ETA: 3:47 - loss: 0.0132 - dice_coef: 0.986 - ETA: 3:46 - loss: 0.0132 - dice_coef: 0.986 - ETA: 3:46 - loss: 0.0131 - dice_coef: 0.986 - ETA: 3:45 - loss: 0.0130 - dice_coef: 0.987 - ETA: 3:44 - loss: 0.0130 - dice_coef: 0.987 - ETA: 3:44 - loss: 0.0129 - dice_coef: 0.987 - ETA: 3:43 - loss: 0.0128 - dice_coef: 0.987 - ETA: 3:42 - loss: 0.0128 - dice_coef: 0.987 - ETA: 3:42 - loss: 0.0127 - dice_coef: 0.987 - ETA: 3:41 - loss: 0.0127 - dice_coef: 0.987 - ETA: 3:40 - loss: 0.0126 - dice_coef: 0.987 - ETA: 3:40 - loss: 0.0125 - dice_coef: 0.987 - ETA: 3:39 - loss: 0.0125 - dice_coef: 0.987 - ETA: 3:38 - loss: 0.0125 - dice_coef: 0.987 - ETA: 3:38 - loss: 0.0124 - dice_coef: 0.987 - ETA: 3:37 - loss: 0.0123 - dice_coef: 0.987 - ETA: 3:36 - loss: 0.0123 - dice_coef: 0.987 - ETA: 3:36 - loss: 0.0122 - dice_coef: 0.987 - ETA: 3:35 - loss: 0.0122 - dice_coef: 0.987 - ETA: 3:35 - loss: 0.0121 - dice_coef: 0.987 - ETA: 3:34 - loss: 0.0120 - dice_coef: 0.988 - ETA: 3:33 - loss: 0.0120 - dice_coef: 0.988 - ETA: 3:33 - loss: 0.0120 - dice_coef: 0.988 - ETA: 3:32 - loss: 0.0119 - dice_coef: 0.988 - ETA: 3:31 - loss: 0.0118 - dice_coef: 0.988 - ETA: 3:31 - loss: 0.0118 - dice_coef: 0.988 - ETA: 3:30 - loss: 0.0118 - dice_coef: 0.988 - ETA: 3:29 - loss: 0.0117 - dice_coef: 0.988 - ETA: 3:29 - loss: 0.0117 - dice_coef: 0.988 - ETA: 3:28 - loss: 0.0116 - dice_coef: 0.988 - ETA: 3:28 - loss: 0.0116 - dice_coef: 0.988 - ETA: 3:27 - loss: 0.0115 - dice_coef: 0.988 - ETA: 3:26 - loss: 0.0115 - dice_coef: 0.988 - ETA: 3:26 - loss: 0.0114 - dice_coef: 0.988 - ETA: 3:25 - loss: 0.0114 - dice_coef: 0.988 - ETA: 3:24 - loss: 0.0113 - dice_coef: 0.988 - ETA: 3:24 - loss: 0.0113 - dice_coef: 0.988 - ETA: 3:23 - loss: 0.0113 - dice_coef: 0.988 - ETA: 3:23 - loss: 0.0113 - dice_coef: 0.988 - ETA: 3:22 - loss: 0.0112 - dice_coef: 0.988 - ETA: 3:21 - loss: 0.0112 - dice_coef: 0.988 - ETA: 3:21 - loss: 0.0112 - dice_coef: 0.988 - ETA: 3:20 - loss: 0.0112 - dice_coef: 0.988 - ETA: 3:20 - loss: 0.0111 - dice_coef: 0.988 - ETA: 3:19 - loss: 0.0111 - dice_coef: 0.988 - ETA: 3:18 - loss: 0.0110 - dice_coef: 0.989 - ETA: 3:18 - loss: 0.0110 - dice_coef: 0.989 - ETA: 3:17 - loss: 0.0110 - dice_coef: 0.989 - ETA: 3:17 - loss: 0.0110 - dice_coef: 0.989 - ETA: 3:16 - loss: 0.0110 - dice_coef: 0.989 - ETA: 3:15 - loss: 0.0110 - dice_coef: 0.989 - ETA: 3:15 - loss: 0.0111 - dice_coef: 0.988 - ETA: 3:14 - loss: 0.0110 - dice_coef: 0.989 - ETA: 3:14 - loss: 0.0110 - dice_coef: 0.989 - ETA: 3:13 - loss: 0.0110 - dice_coef: 0.989 - ETA: 3:12 - loss: 0.0109 - dice_coef: 0.989 - ETA: 3:12 - loss: 0.0109 - dice_coef: 0.989 - ETA: 3:11 - loss: 0.0108 - dice_coef: 0.989 - ETA: 3:11 - loss: 0.0108 - dice_coef: 0.989 - ETA: 3:10 - loss: 0.0107 - dice_coef: 0.989 - ETA: 3:09 - loss: 0.0107 - dice_coef: 0.989 - ETA: 3:09 - loss: 0.0107 - dice_coef: 0.989 - ETA: 3:08 - loss: 0.0106 - dice_coef: 0.989 - ETA: 3:08 - loss: 0.0106 - dice_coef: 0.989 - ETA: 3:07 - loss: 0.0106 - dice_coef: 0.989 - ETA: 3:06 - loss: 0.0105 - dice_coef: 0.989 - ETA: 3:06 - loss: 0.0106 - dice_coef: 0.989 - ETA: 3:05 - loss: 0.0106 - dice_coef: 0.989 - ETA: 3:05 - loss: 0.0105 - dice_coef: 0.989 - ETA: 3:04 - loss: 0.0105 - dice_coef: 0.989 - ETA: 3:04 - loss: 0.0105 - dice_coef: 0.989 - ETA: 3:03 - loss: 0.0105 - dice_coef: 0.989 - ETA: 3:02 - loss: 0.0104 - dice_coef: 0.989 - ETA: 3:02 - loss: 0.0105 - dice_coef: 0.989 - ETA: 3:01 - loss: 0.0104 - dice_coef: 0.989 - ETA: 3:01 - loss: 0.0104 - dice_coef: 0.989 - ETA: 3:00 - loss: 0.0103 - dice_coef: 0.989 - ETA: 2:59 - loss: 0.0104 - dice_coef: 0.989 - ETA: 2:59 - loss: 0.0104 - dice_coef: 0.989 - ETA: 2:58 - loss: 0.0104 - dice_coef: 0.989 - ETA: 2:58 - loss: 0.0103 - dice_coef: 0.989 - ETA: 2:57 - loss: 0.0104 - dice_coef: 0.989 - ETA: 2:57 - loss: 0.0104 - dice_coef: 0.989 - ETA: 2:56 - loss: 0.0104 - dice_coef: 0.989 - ETA: 2:55 - loss: 0.0103 - dice_coef: 0.989 - ETA: 2:55 - loss: 0.0103 - dice_coef: 0.989 - ETA: 2:54 - loss: 0.0103 - dice_coef: 0.989 - ETA: 2:54 - loss: 0.0103 - dice_coef: 0.989 - ETA: 2:53 - loss: 0.0104 - dice_coef: 0.989 - ETA: 2:53 - loss: 0.0103 - dice_coef: 0.989 - ETA: 2:52 - loss: 0.0103 - dice_coef: 0.989 - ETA: 2:51 - loss: 0.0102 - dice_coef: 0.989 - ETA: 2:51 - loss: 0.0102 - dice_coef: 0.989 - ETA: 2:50 - loss: 0.0102 - dice_coef: 0.989 - ETA: 2:50 - loss: 0.0101 - dice_coef: 0.989 - ETA: 2:49 - loss: 0.0101 - dice_coef: 0.989 - ETA: 2:49 - loss: 0.0101 - dice_coef: 0.989 - ETA: 2:48 - loss: 0.0100 - dice_coef: 0.990 - ETA: 2:48 - loss: 0.0100 - dice_coef: 0.990 - ETA: 2:47 - loss: 0.0100 - dice_coef: 0.990 - ETA: 2:46 - loss: 0.0099 - dice_coef: 0.990 - ETA: 2:46 - loss: 0.0099 - dice_coef: 0.990 - ETA: 2:45 - loss: 0.0099 - dice_coef: 0.990 - ETA: 2:45 - loss: 0.0099 - dice_coef: 0.990 - ETA: 2:44 - loss: 0.0098 - dice_coef: 0.990 - ETA: 2:44 - loss: 0.0098 - dice_coef: 0.990 - ETA: 2:43 - loss: 0.0097 - dice_coef: 0.990 - ETA: 2:42 - loss: 0.0097 - dice_coef: 0.990 - ETA: 2:42 - loss: 0.0097 - dice_coef: 0.990 - ETA: 2:41 - loss: 0.0097 - dice_coef: 0.990 - ETA: 2:41 - loss: 0.0096 - dice_coef: 0.990 - ETA: 2:40 - loss: 0.0096 - dice_coef: 0.990 - ETA: 2:40 - loss: 0.0096 - dice_coef: 0.990 - ETA: 2:39 - loss: 0.0095 - dice_coef: 0.990 - ETA: 2:39 - loss: 0.0095 - dice_coef: 0.990 - ETA: 2:38 - loss: 0.0095 - dice_coef: 0.990 - ETA: 2:37 - loss: 0.0095 - dice_coef: 0.990 - ETA: 2:37 - loss: 0.0094 - dice_coef: 0.990 - ETA: 2:36 - loss: 0.0094 - dice_coef: 0.990 - ETA: 2:36 - loss: 0.0094 - dice_coef: 0.990 - ETA: 2:35 - loss: 0.0094 - dice_coef: 0.990 - ETA: 2:35 - loss: 0.0093 - dice_coef: 0.990 - ETA: 2:34 - loss: 0.0094 - dice_coef: 0.990 - ETA: 2:34 - loss: 0.0094 - dice_coef: 0.990 - ETA: 2:33 - loss: 0.0095 - dice_coef: 0.990 - ETA: 2:33 - loss: 0.0094 - dice_coef: 0.990 - ETA: 2:32 - loss: 0.0094 - dice_coef: 0.990 - ETA: 2:31 - loss: 0.0094 - dice_coef: 0.990 - ETA: 2:31 - loss: 0.0094 - dice_coef: 0.990 - ETA: 2:30 - loss: 0.0093 - dice_coef: 0.990 - ETA: 2:30 - loss: 0.0093 - dice_coef: 0.990 - ETA: 2:29 - loss: 0.0093 - dice_coef: 0.990 - ETA: 2:29 - loss: 0.0093 - dice_coef: 0.990 - ETA: 2:28 - loss: 0.0092 - dice_coef: 0.990 - ETA: 2:28 - loss: 0.0092 - dice_coef: 0.990 - ETA: 2:27 - loss: 0.0092 - dice_coef: 0.990 - ETA: 2:27 - loss: 0.0091 - dice_coef: 0.990 - ETA: 2:26 - loss: 0.0091 - dice_coef: 0.990 - ETA: 2:25 - loss: 0.0091 - dice_coef: 0.990 - ETA: 2:25 - loss: 0.0091 - dice_coef: 0.990 - ETA: 2:24 - loss: 0.0090 - dice_coef: 0.991 - ETA: 2:24 - loss: 0.0090 - dice_coef: 0.991 - ETA: 2:23 - loss: 0.0090 - dice_coef: 0.991 - ETA: 2:23 - loss: 0.0090 - dice_coef: 0.991 - ETA: 2:22 - loss: 0.0090 - dice_coef: 0.991 - ETA: 2:22 - loss: 0.0089 - dice_coef: 0.991 - ETA: 2:21 - loss: 0.0089 - dice_coef: 0.991 - ETA: 2:21 - loss: 0.0089 - dice_coef: 0.991 - ETA: 2:20 - loss: 0.0089 - dice_coef: 0.991 - ETA: 2:19 - loss: 0.0088 - dice_coef: 0.991 - ETA: 2:19 - loss: 0.0088 - dice_coef: 0.991 - ETA: 2:18 - loss: 0.0088 - dice_coef: 0.991 - ETA: 2:18 - loss: 0.0088 - dice_coef: 0.991 - ETA: 2:17 - loss: 0.0088 - dice_coef: 0.991 - ETA: 2:17 - loss: 0.0088 - dice_coef: 0.991 - ETA: 2:16 - loss: 0.0087 - dice_coef: 0.991 - ETA: 2:16 - loss: 0.0087 - dice_coef: 0.991 - ETA: 2:15 - loss: 0.0087 - dice_coef: 0.991 - ETA: 2:15 - loss: 0.0087 - dice_coef: 0.991 - ETA: 2:14 - loss: 0.0087 - dice_coef: 0.991 - ETA: 2:14 - loss: 0.0087 - dice_coef: 0.991 - ETA: 2:13 - loss: 0.0086 - dice_coef: 0.991 - ETA: 2:13 - loss: 0.0086 - dice_coef: 0.991 - ETA: 2:12 - loss: 0.0086 - dice_coef: 0.991 - ETA: 2:11 - loss: 0.0086 - dice_coef: 0.991 - ETA: 2:11 - loss: 0.0086 - dice_coef: 0.991 - ETA: 2:10 - loss: 0.0086 - dice_coef: 0.9914"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "534/612 [=========================>....] - ETA: 2:10 - loss: 0.0086 - dice_coef: 0.991 - ETA: 2:09 - loss: 0.0086 - dice_coef: 0.991 - ETA: 2:09 - loss: 0.0086 - dice_coef: 0.991 - ETA: 2:08 - loss: 0.0086 - dice_coef: 0.991 - ETA: 2:08 - loss: 0.0086 - dice_coef: 0.991 - ETA: 2:07 - loss: 0.0086 - dice_coef: 0.991 - ETA: 2:07 - loss: 0.0085 - dice_coef: 0.991 - ETA: 2:06 - loss: 0.0085 - dice_coef: 0.991 - ETA: 2:06 - loss: 0.0085 - dice_coef: 0.991 - ETA: 2:05 - loss: 0.0085 - dice_coef: 0.991 - ETA: 2:05 - loss: 0.0085 - dice_coef: 0.991 - ETA: 2:04 - loss: 0.0084 - dice_coef: 0.991 - ETA: 2:03 - loss: 0.0084 - dice_coef: 0.991 - ETA: 2:03 - loss: 0.0084 - dice_coef: 0.991 - ETA: 2:02 - loss: 0.0084 - dice_coef: 0.991 - ETA: 2:02 - loss: 0.0084 - dice_coef: 0.991 - ETA: 2:01 - loss: 0.0084 - dice_coef: 0.991 - ETA: 2:01 - loss: 0.0083 - dice_coef: 0.991 - ETA: 2:00 - loss: 0.0083 - dice_coef: 0.991 - ETA: 2:00 - loss: 0.0083 - dice_coef: 0.991 - ETA: 1:59 - loss: 0.0083 - dice_coef: 0.991 - ETA: 1:59 - loss: 0.0083 - dice_coef: 0.991 - ETA: 1:58 - loss: 0.0083 - dice_coef: 0.991 - ETA: 1:58 - loss: 0.0082 - dice_coef: 0.991 - ETA: 1:57 - loss: 0.0082 - dice_coef: 0.991 - ETA: 1:57 - loss: 0.0082 - dice_coef: 0.991 - ETA: 1:56 - loss: 0.0082 - dice_coef: 0.991 - ETA: 1:56 - loss: 0.0082 - dice_coef: 0.991 - ETA: 1:55 - loss: 0.0081 - dice_coef: 0.991 - ETA: 1:55 - loss: 0.0081 - dice_coef: 0.991 - ETA: 1:54 - loss: 0.0081 - dice_coef: 0.991 - ETA: 1:53 - loss: 0.0081 - dice_coef: 0.991 - ETA: 1:53 - loss: 0.0081 - dice_coef: 0.991 - ETA: 1:52 - loss: 0.0080 - dice_coef: 0.992 - ETA: 1:52 - loss: 0.0080 - dice_coef: 0.992 - ETA: 1:51 - loss: 0.0080 - dice_coef: 0.992 - ETA: 1:51 - loss: 0.0080 - dice_coef: 0.992 - ETA: 1:50 - loss: 0.0080 - dice_coef: 0.992 - ETA: 1:50 - loss: 0.0080 - dice_coef: 0.992 - ETA: 1:49 - loss: 0.0079 - dice_coef: 0.992 - ETA: 1:49 - loss: 0.0080 - dice_coef: 0.992 - ETA: 1:48 - loss: 0.0080 - dice_coef: 0.992 - ETA: 1:48 - loss: 0.0080 - dice_coef: 0.992 - ETA: 1:47 - loss: 0.0079 - dice_coef: 0.992 - ETA: 1:47 - loss: 0.0079 - dice_coef: 0.992 - ETA: 1:46 - loss: 0.0079 - dice_coef: 0.992 - ETA: 1:46 - loss: 0.0079 - dice_coef: 0.992 - ETA: 1:45 - loss: 0.0079 - dice_coef: 0.992 - ETA: 1:45 - loss: 0.0079 - dice_coef: 0.992 - ETA: 1:44 - loss: 0.0078 - dice_coef: 0.992 - ETA: 1:44 - loss: 0.0078 - dice_coef: 0.992 - ETA: 1:43 - loss: 0.0078 - dice_coef: 0.992 - ETA: 1:43 - loss: 0.0078 - dice_coef: 0.992 - ETA: 1:42 - loss: 0.0078 - dice_coef: 0.992 - ETA: 1:41 - loss: 0.0077 - dice_coef: 0.992 - ETA: 1:41 - loss: 0.0077 - dice_coef: 0.992 - ETA: 1:40 - loss: 0.0077 - dice_coef: 0.992 - ETA: 1:40 - loss: 0.0077 - dice_coef: 0.992 - ETA: 1:39 - loss: 0.0077 - dice_coef: 0.992 - ETA: 1:39 - loss: 0.0077 - dice_coef: 0.992 - ETA: 1:38 - loss: 0.0077 - dice_coef: 0.992 - ETA: 1:38 - loss: 0.0076 - dice_coef: 0.992 - ETA: 1:37 - loss: 0.0076 - dice_coef: 0.992 - ETA: 1:37 - loss: 0.0076 - dice_coef: 0.992 - ETA: 1:36 - loss: 0.0076 - dice_coef: 0.992 - ETA: 1:36 - loss: 0.0076 - dice_coef: 0.992 - ETA: 1:35 - loss: 0.0076 - dice_coef: 0.992 - ETA: 1:35 - loss: 0.0076 - dice_coef: 0.992 - ETA: 1:34 - loss: 0.0076 - dice_coef: 0.992 - ETA: 1:34 - loss: 0.0075 - dice_coef: 0.992 - ETA: 1:33 - loss: 0.0075 - dice_coef: 0.992 - ETA: 1:33 - loss: 0.0075 - dice_coef: 0.992 - ETA: 1:32 - loss: 0.0075 - dice_coef: 0.992 - ETA: 1:32 - loss: 0.0075 - dice_coef: 0.992 - ETA: 1:31 - loss: 0.0075 - dice_coef: 0.992 - ETA: 1:31 - loss: 0.0074 - dice_coef: 0.992 - ETA: 1:30 - loss: 0.0074 - dice_coef: 0.992 - ETA: 1:30 - loss: 0.0074 - dice_coef: 0.992 - ETA: 1:29 - loss: 0.0074 - dice_coef: 0.992 - ETA: 1:29 - loss: 0.0074 - dice_coef: 0.992 - ETA: 1:28 - loss: 0.0074 - dice_coef: 0.992 - ETA: 1:28 - loss: 0.0073 - dice_coef: 0.992 - ETA: 1:27 - loss: 0.0073 - dice_coef: 0.992 - ETA: 1:27 - loss: 0.0073 - dice_coef: 0.992 - ETA: 1:26 - loss: 0.0073 - dice_coef: 0.992 - ETA: 1:26 - loss: 0.0073 - dice_coef: 0.992 - ETA: 1:25 - loss: 0.0073 - dice_coef: 0.992 - ETA: 1:24 - loss: 0.0073 - dice_coef: 0.992 - ETA: 1:24 - loss: 0.0073 - dice_coef: 0.992 - ETA: 1:23 - loss: 0.0073 - dice_coef: 0.992 - ETA: 1:23 - loss: 0.0073 - dice_coef: 0.992 - ETA: 1:22 - loss: 0.0073 - dice_coef: 0.992 - ETA: 1:22 - loss: 0.0073 - dice_coef: 0.992 - ETA: 1:21 - loss: 0.0072 - dice_coef: 0.992 - ETA: 1:21 - loss: 0.0072 - dice_coef: 0.992 - ETA: 1:20 - loss: 0.0073 - dice_coef: 0.992 - ETA: 1:20 - loss: 0.0072 - dice_coef: 0.992 - ETA: 1:19 - loss: 0.0072 - dice_coef: 0.992 - ETA: 1:19 - loss: 0.0072 - dice_coef: 0.992 - ETA: 1:18 - loss: 0.0072 - dice_coef: 0.992 - ETA: 1:18 - loss: 0.0072 - dice_coef: 0.992 - ETA: 1:17 - loss: 0.0072 - dice_coef: 0.992 - ETA: 1:17 - loss: 0.0071 - dice_coef: 0.992 - ETA: 1:16 - loss: 0.0071 - dice_coef: 0.992 - ETA: 1:16 - loss: 0.0071 - dice_coef: 0.992 - ETA: 1:15 - loss: 0.0071 - dice_coef: 0.992 - ETA: 1:15 - loss: 0.0071 - dice_coef: 0.992 - ETA: 1:14 - loss: 0.0071 - dice_coef: 0.992 - ETA: 1:14 - loss: 0.0071 - dice_coef: 0.992 - ETA: 1:13 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:13 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:12 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:12 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:11 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:11 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:10 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:10 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:09 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:09 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:08 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:08 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:07 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:07 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:06 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:06 - loss: 0.0069 - dice_coef: 0.993 - ETA: 1:05 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:05 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:04 - loss: 0.0069 - dice_coef: 0.993 - ETA: 1:04 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:03 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:03 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:02 - loss: 0.0071 - dice_coef: 0.992 - ETA: 1:02 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:01 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:01 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:00 - loss: 0.0070 - dice_coef: 0.993 - ETA: 1:00 - loss: 0.0070 - dice_coef: 0.993 - ETA: 59s - loss: 0.0070 - dice_coef: 0.993 - ETA: 59s - loss: 0.0070 - dice_coef: 0.99 - ETA: 58s - loss: 0.0070 - dice_coef: 0.99 - ETA: 58s - loss: 0.0070 - dice_coef: 0.99 - ETA: 57s - loss: 0.0070 - dice_coef: 0.99 - ETA: 57s - loss: 0.0070 - dice_coef: 0.99 - ETA: 56s - loss: 0.0070 - dice_coef: 0.99 - ETA: 56s - loss: 0.0070 - dice_coef: 0.99 - ETA: 55s - loss: 0.0070 - dice_coef: 0.99 - ETA: 55s - loss: 0.0069 - dice_coef: 0.99 - ETA: 54s - loss: 0.0069 - dice_coef: 0.99 - ETA: 53s - loss: 0.0069 - dice_coef: 0.99 - ETA: 53s - loss: 0.0069 - dice_coef: 0.99 - ETA: 52s - loss: 0.0069 - dice_coef: 0.99 - ETA: 52s - loss: 0.0069 - dice_coef: 0.99 - ETA: 51s - loss: 0.0069 - dice_coef: 0.99 - ETA: 51s - loss: 0.0069 - dice_coef: 0.99 - ETA: 50s - loss: 0.0069 - dice_coef: 0.99 - ETA: 50s - loss: 0.0069 - dice_coef: 0.99 - ETA: 49s - loss: 0.0069 - dice_coef: 0.99 - ETA: 49s - loss: 0.0069 - dice_coef: 0.99 - ETA: 48s - loss: 0.0069 - dice_coef: 0.99 - ETA: 48s - loss: 0.0069 - dice_coef: 0.99 - ETA: 47s - loss: 0.0069 - dice_coef: 0.99 - ETA: 47s - loss: 0.0069 - dice_coef: 0.99 - ETA: 46s - loss: 0.0069 - dice_coef: 0.99 - ETA: 46s - loss: 0.0068 - dice_coef: 0.99 - ETA: 45s - loss: 0.0069 - dice_coef: 0.99 - ETA: 45s - loss: 0.0069 - dice_coef: 0.99 - ETA: 44s - loss: 0.0069 - dice_coef: 0.99 - ETA: 44s - loss: 0.0069 - dice_coef: 0.99 - ETA: 43s - loss: 0.0069 - dice_coef: 0.99 - ETA: 43s - loss: 0.0069 - dice_coef: 0.99 - ETA: 42s - loss: 0.0069 - dice_coef: 0.99 - ETA: 42s - loss: 0.0069 - dice_coef: 0.99 - ETA: 41s - loss: 0.0069 - dice_coef: 0.99 - ETA: 41s - loss: 0.0069 - dice_coef: 0.99 - ETA: 40s - loss: 0.0069 - dice_coef: 0.99 - ETA: 40s - loss: 0.0069 - dice_coef: 0.99 - ETA: 39s - loss: 0.0069 - dice_coef: 0.99 - ETA: 39s - loss: 0.0069 - dice_coef: 0.99 - ETA: 38s - loss: 0.0068 - dice_coef: 0.9932"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612/612 [==============================] - ETA: 38s - loss: 0.0068 - dice_coef: 0.99 - ETA: 37s - loss: 0.0068 - dice_coef: 0.99 - ETA: 37s - loss: 0.0068 - dice_coef: 0.99 - ETA: 36s - loss: 0.0068 - dice_coef: 0.99 - ETA: 36s - loss: 0.0068 - dice_coef: 0.99 - ETA: 35s - loss: 0.0068 - dice_coef: 0.99 - ETA: 35s - loss: 0.0068 - dice_coef: 0.99 - ETA: 34s - loss: 0.0068 - dice_coef: 0.99 - ETA: 34s - loss: 0.0068 - dice_coef: 0.99 - ETA: 33s - loss: 0.0068 - dice_coef: 0.99 - ETA: 33s - loss: 0.0068 - dice_coef: 0.99 - ETA: 32s - loss: 0.0067 - dice_coef: 0.99 - ETA: 32s - loss: 0.0067 - dice_coef: 0.99 - ETA: 31s - loss: 0.0067 - dice_coef: 0.99 - ETA: 31s - loss: 0.0067 - dice_coef: 0.99 - ETA: 30s - loss: 0.0067 - dice_coef: 0.99 - ETA: 30s - loss: 0.0068 - dice_coef: 0.99 - ETA: 29s - loss: 0.0068 - dice_coef: 0.99 - ETA: 29s - loss: 0.0068 - dice_coef: 0.99 - ETA: 28s - loss: 0.0068 - dice_coef: 0.99 - ETA: 28s - loss: 0.0067 - dice_coef: 0.99 - ETA: 27s - loss: 0.0068 - dice_coef: 0.99 - ETA: 27s - loss: 0.0067 - dice_coef: 0.99 - ETA: 26s - loss: 0.0067 - dice_coef: 0.99 - ETA: 26s - loss: 0.0067 - dice_coef: 0.99 - ETA: 25s - loss: 0.0067 - dice_coef: 0.99 - ETA: 25s - loss: 0.0067 - dice_coef: 0.99 - ETA: 24s - loss: 0.0067 - dice_coef: 0.99 - ETA: 24s - loss: 0.0067 - dice_coef: 0.99 - ETA: 23s - loss: 0.0067 - dice_coef: 0.99 - ETA: 23s - loss: 0.0066 - dice_coef: 0.99 - ETA: 22s - loss: 0.0067 - dice_coef: 0.99 - ETA: 22s - loss: 0.0067 - dice_coef: 0.99 - ETA: 21s - loss: 0.0067 - dice_coef: 0.99 - ETA: 21s - loss: 0.0067 - dice_coef: 0.99 - ETA: 20s - loss: 0.0067 - dice_coef: 0.99 - ETA: 20s - loss: 0.0067 - dice_coef: 0.99 - ETA: 19s - loss: 0.0067 - dice_coef: 0.99 - ETA: 19s - loss: 0.0067 - dice_coef: 0.99 - ETA: 18s - loss: 0.0067 - dice_coef: 0.99 - ETA: 18s - loss: 0.0067 - dice_coef: 0.99 - ETA: 17s - loss: 0.0067 - dice_coef: 0.99 - ETA: 17s - loss: 0.0067 - dice_coef: 0.99 - ETA: 16s - loss: 0.0067 - dice_coef: 0.99 - ETA: 16s - loss: 0.0067 - dice_coef: 0.99 - ETA: 15s - loss: 0.0067 - dice_coef: 0.99 - ETA: 15s - loss: 0.0067 - dice_coef: 0.99 - ETA: 14s - loss: 0.0067 - dice_coef: 0.99 - ETA: 14s - loss: 0.0067 - dice_coef: 0.99 - ETA: 13s - loss: 0.0067 - dice_coef: 0.99 - ETA: 13s - loss: 0.0067 - dice_coef: 0.99 - ETA: 12s - loss: 0.0067 - dice_coef: 0.99 - ETA: 12s - loss: 0.0067 - dice_coef: 0.99 - ETA: 11s - loss: 0.0067 - dice_coef: 0.99 - ETA: 11s - loss: 0.0066 - dice_coef: 0.99 - ETA: 10s - loss: 0.0066 - dice_coef: 0.99 - ETA: 10s - loss: 0.0066 - dice_coef: 0.99 - ETA: 9s - loss: 0.0066 - dice_coef: 0.9934 - ETA: 9s - loss: 0.0066 - dice_coef: 0.993 - ETA: 8s - loss: 0.0066 - dice_coef: 0.993 - ETA: 8s - loss: 0.0066 - dice_coef: 0.993 - ETA: 7s - loss: 0.0066 - dice_coef: 0.993 - ETA: 7s - loss: 0.0066 - dice_coef: 0.993 - ETA: 6s - loss: 0.0066 - dice_coef: 0.993 - ETA: 6s - loss: 0.0066 - dice_coef: 0.993 - ETA: 5s - loss: 0.0066 - dice_coef: 0.993 - ETA: 5s - loss: 0.0066 - dice_coef: 0.993 - ETA: 4s - loss: 0.0066 - dice_coef: 0.993 - ETA: 4s - loss: 0.0066 - dice_coef: 0.993 - ETA: 3s - loss: 0.0066 - dice_coef: 0.993 - ETA: 3s - loss: 0.0066 - dice_coef: 0.993 - ETA: 2s - loss: 0.0066 - dice_coef: 0.993 - ETA: 2s - loss: 0.0066 - dice_coef: 0.993 - ETA: 1s - loss: 0.0066 - dice_coef: 0.993 - ETA: 1s - loss: 0.0066 - dice_coef: 0.993 - ETA: 0s - loss: 0.0066 - dice_coef: 0.993 - ETA: 0s - loss: 0.0066 - dice_coef: 0.993 - 304s 497ms/step - loss: 0.0066 - dice_coef: 0.9934\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/612 [======>.......................] - ETA: 4:54 - loss: 0.0011 - dice_coef: 0.998 - ETA: 4:57 - loss: 5.3566e-04 - dice_coef: 0.999 - ETA: 4:55 - loss: 0.0066 - dice_coef: 0.9934    - ETA: 4:56 - loss: 0.0085 - dice_coef: 0.991 - ETA: 4:56 - loss: 0.0068 - dice_coef: 0.993 - ETA: 4:55 - loss: 0.0057 - dice_coef: 0.994 - ETA: 4:54 - loss: 0.0049 - dice_coef: 0.995 - ETA: 4:54 - loss: 0.0043 - dice_coef: 0.995 - ETA: 4:53 - loss: 0.0038 - dice_coef: 0.996 - ETA: 4:52 - loss: 0.0042 - dice_coef: 0.995 - ETA: 4:52 - loss: 0.0038 - dice_coef: 0.996 - ETA: 4:51 - loss: 0.0049 - dice_coef: 0.995 - ETA: 4:50 - loss: 0.0045 - dice_coef: 0.995 - ETA: 4:50 - loss: 0.0042 - dice_coef: 0.995 - ETA: 4:49 - loss: 0.0039 - dice_coef: 0.996 - ETA: 4:49 - loss: 0.0036 - dice_coef: 0.996 - ETA: 4:48 - loss: 0.0034 - dice_coef: 0.996 - ETA: 4:48 - loss: 0.0032 - dice_coef: 0.996 - ETA: 4:48 - loss: 0.0031 - dice_coef: 0.996 - ETA: 4:47 - loss: 0.0029 - dice_coef: 0.997 - ETA: 4:46 - loss: 0.0028 - dice_coef: 0.997 - ETA: 4:46 - loss: 0.0026 - dice_coef: 0.997 - ETA: 4:45 - loss: 0.0026 - dice_coef: 0.997 - ETA: 4:44 - loss: 0.0025 - dice_coef: 0.997 - ETA: 4:44 - loss: 0.0024 - dice_coef: 0.997 - ETA: 4:43 - loss: 0.0023 - dice_coef: 0.997 - ETA: 4:43 - loss: 0.0022 - dice_coef: 0.997 - ETA: 4:43 - loss: 0.0022 - dice_coef: 0.997 - ETA: 4:42 - loss: 0.0021 - dice_coef: 0.997 - ETA: 4:41 - loss: 0.0020 - dice_coef: 0.998 - ETA: 4:41 - loss: 0.0019 - dice_coef: 0.998 - ETA: 4:40 - loss: 0.0019 - dice_coef: 0.998 - ETA: 4:40 - loss: 0.0018 - dice_coef: 0.998 - ETA: 4:39 - loss: 0.0018 - dice_coef: 0.998 - ETA: 4:39 - loss: 0.0017 - dice_coef: 0.998 - ETA: 4:38 - loss: 0.0018 - dice_coef: 0.998 - ETA: 4:38 - loss: 0.0017 - dice_coef: 0.998 - ETA: 4:37 - loss: 0.0019 - dice_coef: 0.998 - ETA: 4:37 - loss: 0.0019 - dice_coef: 0.998 - ETA: 4:36 - loss: 0.0018 - dice_coef: 0.998 - ETA: 4:36 - loss: 0.0018 - dice_coef: 0.998 - ETA: 4:35 - loss: 0.0018 - dice_coef: 0.998 - ETA: 4:34 - loss: 0.0018 - dice_coef: 0.998 - ETA: 4:34 - loss: 0.0017 - dice_coef: 0.998 - ETA: 4:33 - loss: 0.0017 - dice_coef: 0.998 - ETA: 4:33 - loss: 0.0019 - dice_coef: 0.998 - ETA: 4:32 - loss: 0.0019 - dice_coef: 0.998 - ETA: 4:32 - loss: 0.0026 - dice_coef: 0.997 - ETA: 4:31 - loss: 0.0026 - dice_coef: 0.997 - ETA: 4:31 - loss: 0.0026 - dice_coef: 0.997 - ETA: 4:30 - loss: 0.0025 - dice_coef: 0.997 - ETA: 4:30 - loss: 0.0026 - dice_coef: 0.997 - ETA: 4:29 - loss: 0.0026 - dice_coef: 0.997 - ETA: 4:29 - loss: 0.0025 - dice_coef: 0.997 - ETA: 4:28 - loss: 0.0025 - dice_coef: 0.997 - ETA: 4:28 - loss: 0.0024 - dice_coef: 0.997 - ETA: 4:27 - loss: 0.0026 - dice_coef: 0.997 - ETA: 4:27 - loss: 0.0026 - dice_coef: 0.997 - ETA: 4:26 - loss: 0.0025 - dice_coef: 0.997 - ETA: 4:26 - loss: 0.0025 - dice_coef: 0.997 - ETA: 4:25 - loss: 0.0026 - dice_coef: 0.997 - ETA: 4:25 - loss: 0.0026 - dice_coef: 0.997 - ETA: 4:24 - loss: 0.0026 - dice_coef: 0.997 - ETA: 4:24 - loss: 0.0028 - dice_coef: 0.997 - ETA: 4:23 - loss: 0.0027 - dice_coef: 0.997 - ETA: 4:23 - loss: 0.0033 - dice_coef: 0.996 - ETA: 4:22 - loss: 0.0032 - dice_coef: 0.996 - ETA: 4:22 - loss: 0.0032 - dice_coef: 0.996 - ETA: 4:21 - loss: 0.0031 - dice_coef: 0.996 - ETA: 4:21 - loss: 0.0034 - dice_coef: 0.996 - ETA: 4:20 - loss: 0.0033 - dice_coef: 0.996 - ETA: 4:20 - loss: 0.0034 - dice_coef: 0.996 - ETA: 4:19 - loss: 0.0034 - dice_coef: 0.996 - ETA: 4:19 - loss: 0.0034 - dice_coef: 0.996 - ETA: 4:18 - loss: 0.0033 - dice_coef: 0.996 - ETA: 4:18 - loss: 0.0035 - dice_coef: 0.996 - ETA: 4:17 - loss: 0.0035 - dice_coef: 0.996 - ETA: 4:17 - loss: 0.0034 - dice_coef: 0.996 - ETA: 4:16 - loss: 0.0036 - dice_coef: 0.996 - ETA: 4:16 - loss: 0.0035 - dice_coef: 0.996 - ETA: 4:15 - loss: 0.0035 - dice_coef: 0.996 - ETA: 4:15 - loss: 0.0035 - dice_coef: 0.996 - ETA: 4:14 - loss: 0.0034 - dice_coef: 0.996 - ETA: 4:14 - loss: 0.0034 - dice_coef: 0.996 - ETA: 4:13 - loss: 0.0034 - dice_coef: 0.996 - ETA: 4:13 - loss: 0.0034 - dice_coef: 0.996 - ETA: 4:13 - loss: 0.0033 - dice_coef: 0.996 - ETA: 4:12 - loss: 0.0033 - dice_coef: 0.996 - ETA: 4:12 - loss: 0.0033 - dice_coef: 0.996 - ETA: 4:11 - loss: 0.0033 - dice_coef: 0.996 - ETA: 4:11 - loss: 0.0033 - dice_coef: 0.996 - ETA: 4:10 - loss: 0.0032 - dice_coef: 0.996 - ETA: 4:10 - loss: 0.0033 - dice_coef: 0.996 - ETA: 4:09 - loss: 0.0032 - dice_coef: 0.996 - ETA: 4:09 - loss: 0.0032 - dice_coef: 0.996 - ETA: 4:08 - loss: 0.0032 - dice_coef: 0.996 - ETA: 4:08 - loss: 0.0033 - dice_coef: 0.996 - ETA: 4:07 - loss: 0.0032 - dice_coef: 0.996 - ETA: 4:07 - loss: 0.0033 - dice_coef: 0.996 - ETA: 4:06 - loss: 0.0032 - dice_coef: 0.996 - ETA: 4:06 - loss: 0.0032 - dice_coef: 0.996 - ETA: 4:05 - loss: 0.0032 - dice_coef: 0.996 - ETA: 4:05 - loss: 0.0032 - dice_coef: 0.996 - ETA: 4:04 - loss: 0.0031 - dice_coef: 0.996 - ETA: 4:04 - loss: 0.0032 - dice_coef: 0.996 - ETA: 4:04 - loss: 0.0032 - dice_coef: 0.996 - ETA: 4:03 - loss: 0.0032 - dice_coef: 0.996 - ETA: 4:03 - loss: 0.0031 - dice_coef: 0.996 - ETA: 4:02 - loss: 0.0031 - dice_coef: 0.996 - ETA: 4:02 - loss: 0.0035 - dice_coef: 0.996 - ETA: 4:01 - loss: 0.0037 - dice_coef: 0.996 - ETA: 4:01 - loss: 0.0037 - dice_coef: 0.996 - ETA: 4:00 - loss: 0.0037 - dice_coef: 0.996 - ETA: 4:00 - loss: 0.0037 - dice_coef: 0.996 - ETA: 3:59 - loss: 0.0038 - dice_coef: 0.996 - ETA: 3:59 - loss: 0.0037 - dice_coef: 0.996 - ETA: 3:58 - loss: 0.0039 - dice_coef: 0.996 - ETA: 3:58 - loss: 0.0041 - dice_coef: 0.995 - ETA: 3:57 - loss: 0.0041 - dice_coef: 0.995 - ETA: 3:57 - loss: 0.0041 - dice_coef: 0.995 - ETA: 3:56 - loss: 0.0041 - dice_coef: 0.995 - ETA: 3:56 - loss: 0.0040 - dice_coef: 0.996 - ETA: 3:55 - loss: 0.0040 - dice_coef: 0.996 - ETA: 3:55 - loss: 0.0040 - dice_coef: 0.996 - ETA: 3:54 - loss: 0.0040 - dice_coef: 0.996 - ETA: 3:54 - loss: 0.0040 - dice_coef: 0.996 - ETA: 3:53 - loss: 0.0040 - dice_coef: 0.996 - ETA: 3:53 - loss: 0.0039 - dice_coef: 0.996 - ETA: 3:52 - loss: 0.0040 - dice_coef: 0.996 - ETA: 3:52 - loss: 0.0040 - dice_coef: 0.996 - ETA: 3:51 - loss: 0.0039 - dice_coef: 0.996 - ETA: 3:51 - loss: 0.0039 - dice_coef: 0.996 - ETA: 3:50 - loss: 0.0041 - dice_coef: 0.995 - ETA: 3:50 - loss: 0.0041 - dice_coef: 0.995 - ETA: 3:49 - loss: 0.0042 - dice_coef: 0.995 - ETA: 3:49 - loss: 0.0042 - dice_coef: 0.995 - ETA: 3:48 - loss: 0.0041 - dice_coef: 0.995 - ETA: 3:48 - loss: 0.0042 - dice_coef: 0.995 - ETA: 3:48 - loss: 0.0041 - dice_coef: 0.995 - ETA: 3:47 - loss: 0.0041 - dice_coef: 0.995 - ETA: 3:47 - loss: 0.0041 - dice_coef: 0.995 - ETA: 3:46 - loss: 0.0042 - dice_coef: 0.995 - ETA: 3:46 - loss: 0.0041 - dice_coef: 0.995 - ETA: 3:45 - loss: 0.0041 - dice_coef: 0.995 - ETA: 3:45 - loss: 0.0041 - dice_coef: 0.995 - ETA: 3:44 - loss: 0.0041 - dice_coef: 0.9959"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-2b78081c8814>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainGen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3060\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator = trainGen,steps_per_epoch = 3060 // batch_size, epochs = n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = History()\n",
    "for epoch in range(0, n_epoch):\n",
    "    epoch_time = time.time()\n",
    "    n_batch = 0\n",
    "    print(\"[+] Epoch ==> \",epoch + 1)\n",
    "    for batch in tl.iterate.minibatches(inputs = XTrain, targets = yTrain, batch_size = batch_size, shuffle = True):\n",
    "        images, labels = batch\n",
    "\n",
    "        data = tl.prepro.threading_data([_ for _ in zip(images[:,:,:,0, np.newaxis],\n",
    "                        images[:,:,:,1, np.newaxis], images[:,:,:,2, np.newaxis],\n",
    "                        images[:,:,:,3, np.newaxis], labels)], fn = DistortImages)\n",
    "        bImages = data[:, 0:4, :, :, :]\n",
    "        bLabels = data[:, 4, :, :, :]\n",
    "        bImages = bImages.transpose((0, 2, 3, 1, 4))\n",
    "        bImages.shape = (batch_size, nw, nh, nz)\n",
    "\n",
    "        model.fit(x = bImages, y = labels, batch_size = batch_size, verbose = 0, shuffle = False, callbacks=[train_history])\n",
    "        n_batch += 1\n",
    "        \n",
    "        if n_batch % printFreq == 0:\n",
    "            print(\"batch ==> \", n_batch, \", images parsed ==> \", batch_size * n_batch)\n",
    "            print(train_history.history)\n",
    "    \n",
    "    print(\"[+] Epoch over ==> \", epoch + 1, \" out of \", n_epoch)\n",
    "    print(train_history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_json = model.to_json()\n",
    "with open(\"unet.json\", \"w\") as json_file:\n",
    "    json_file.write(net_json)\n",
    "model.save_weights(\"unet.h5\")\n",
    "print(\"[+] Model Saved in disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test saved model\n",
    "### 1. Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonFile = open('unet.json', \"r\")\n",
    "loadedModelJson = jsonFile.read()\n",
    "jsonFile.close()\n",
    "\n",
    "model = model_from_json(loadedModelJson)\n",
    "model.load_weights(\"unet.h5\")\n",
    "print(\"[+] Model loaded\")\n",
    "model.compile(optimizer = Adam(lr=lr, beta_1=beta1), loss = 'binary_crossentropy', metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x = XTest, y = yTest, batch_size = batch_size, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outPred = model.predict(XTest, batch_size = batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(outPred.shape[0]):\n",
    "    VisualizeImageWithPrediction(XTest[i], yTest[i], outPred[i], \"./test/{}/test_{}.png\".format(task, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outPredTrain = model.predict(XTrain, batch_size = batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(outPred.shape[0]):\n",
    "    VisualizeImageWithPrediction(XTrain[i], yTrain[i], outPred[i], \"./train/{}/train_{}.png\".format(task, i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
