{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, time, logging, traceback\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.utils import Sequence\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from error_handling_utilities import ModelFrameError\n",
    "from error_handling_utilities import logClass as log\n",
    "from DataGenerator import DataGenerator\n",
    "from data_utilities import VisualizeImage, VisualizeImageWithPrediction\n",
    "from model_utilities import LoadModelJSON, SaveModelJSON\n",
    "from loss_utilities import dice_coef_loss, dice_coef, jaccard_distance_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet:\n",
    "\n",
    "    # NOTE: Private functions\n",
    "    def __init__(self, shape:tuple, n_out = 1, dropout = 0.5, batchNorm = False, modelPath = None, modelName = None, loadModel = False):\n",
    "        '''\n",
    "        initialize the UNet model\n",
    "        \n",
    "        Arguments:\n",
    "            shape {tuple of integer} -- the shape describing the input shape\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            n_out {int} -- the output size (default: {1})\n",
    "            dropout {float} -- dropout prob. (default: {0.5})\n",
    "            batchNorm {bool} -- status check to do batch normalization (default: {False})            \n",
    "            modelPath {str} -- path of the saved model to load (default: {None})\n",
    "            modelName {str} -- the name of the model to load (default: {None})\n",
    "            loadModel {bool} -- status check for loading model(default: {False})\n",
    "        '''\n",
    "        # Setup model\n",
    "        self.model = None\n",
    "        if loadModel == True:\n",
    "            self.model = self.LoadModel(path = modelPath, modelName = modelName)\n",
    "            print(\"[INFO] Model loaded. Compile it\")\n",
    "            log.logger.info(\"[INFO] Model Loaded. Compile it!\")\n",
    "\n",
    "        else:\n",
    "            self.model = self.__InitializeModel__(shape = shape, n_out = n_out, dropout = dropout, batchNorm = batchNorm)\n",
    "            print(\"[INFO] Model built. Compile it!\")\n",
    "            log.logger.info(\"[INFO] Model built. Compile it!\")\n",
    "\n",
    "        # Setup folder list\n",
    "        folderList = [\"./checkpoint\", \n",
    "        \"./sample\",\n",
    "        \"./test\",\n",
    "        \"./savedModels\"]\n",
    "        self.__InitializeFolders__(folderList)\n",
    "    \n",
    "    def __InitializeFolders__(self, folderList: list) -> bool:\n",
    "        '''\n",
    "        Function to initialize all the required folders for the model to\n",
    "        run. NOTE: PRIVATE FUNCTION -- DON'T USE\n",
    "        \n",
    "        Arguments:\n",
    "            folderList {list} -- Consist of all the folders to initialize\n",
    "        \n",
    "        Returns:\n",
    "            bool -- status return\n",
    "        '''\n",
    "        for folder in folderList:\n",
    "            try:\n",
    "                if not os.path.exists(folder):\n",
    "                    os.mkdir(folder)\n",
    "            except Exception as err:\n",
    "                print(\"[FATAL] Exception occured while making folders: \", err)\n",
    "                traceback.print_exc()\n",
    "                log.logger.critical(\"[FATAL] Exception occured while making folders: \" + err)\n",
    "                return False\n",
    "        print(\"[INFO] Folders initialized\")\n",
    "        log.logger.info(\"[INFO] Folders initialized\")\n",
    "        return True\n",
    "\n",
    "    def __Conv2DBlock__(self, inputTensor, filters:int, kernelSize = 3, batchNorm = False):\n",
    "        '''\n",
    "        A conv2D block that defines a full 2 layer conv2d with batchnorm \n",
    "        # NOTE: PRIVATE FUNCTION -- DON'T USE\n",
    "\n",
    "        Arguments:\n",
    "            inputTensor {tensor} -- the input tensor\n",
    "            filters {int} -- the total no. of filters\n",
    "\n",
    "        Keyword Arguments:\n",
    "            kernelSize {int} -- the size of each filter (default: {3})\n",
    "            batchNorm {bool} -- status check for batchnorm (default: {False})\n",
    "        '''\n",
    "        # First layer\n",
    "        x = Conv2D(filters = filters, kernel_size = (kernelSize, kernelSize), kernel_initializer = \"he_normal\", padding = \"same\")(inputTensor)\n",
    "        if batchNorm:\n",
    "            x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "\n",
    "        # Second layer\n",
    "        x = Conv2D(filters = filters, kernel_size = (kernelSize, kernelSize), kernel_initializer = \"he_normal\", padding = \"same\")(x)\n",
    "        if batchNorm:\n",
    "            x = BatchNormalization()(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        return x\n",
    "\n",
    "    def __InitializeModel__(self, shape, n_out = 1, dropout = 0.5, batchNorm = False, ) -> Model:\n",
    "        '''\n",
    "        Function to initialize the UNet code # NOTE: PRIVATE FUNCTION -- DON'T USE\n",
    "        \n",
    "        Arguments:\n",
    "            shape {tuple of int} -- shape of the input\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            n_out {int} -- the output size (default: {1})\n",
    "            dropout {float} -- dropout prob. (default: {0.5})\n",
    "            batchNorm {bool} -- status check to do batch normalization (default: {False})\n",
    "        \n",
    "        Returns:\n",
    "            Model -- the builded model -- YET TO COMPILE\n",
    "        '''\n",
    "\n",
    "        nnx = int(shape[1])\n",
    "        nny = int(shape[2])\n",
    "        nnz = int(shape[3])\n",
    "        print(\" * Input: size of image: %d %d %d\" % (nnx, nny, nnz))\n",
    "\n",
    "        # Encoder \n",
    "        inputImage = Input((nnx, nny, nnz))\n",
    "        # level 1\n",
    "        conv1 = self.__Conv2DBlock__(inputTensor = inputImage, filters = 64, kernelSize = 3, batchNorm = batchNorm)\n",
    "        pool1 = MaxPooling2D(pool_size = (2, 2))(conv1)\n",
    "        pool1 = Dropout(dropout * 0.5)(pool1)\n",
    "        # level 2\n",
    "        conv2 = self.__Conv2DBlock__(inputTensor = pool1, filters = 128, kernelSize = 3, batchNorm = batchNorm)\n",
    "        pool2 = MaxPooling2D(pool_size = (2, 2))(conv2)\n",
    "        pool2 = Dropout(dropout)(pool2)\n",
    "        # level 3\n",
    "        conv3 = self.__Conv2DBlock__(inputTensor = pool2, filters = 256, kernelSize = 3, batchNorm = batchNorm)\n",
    "        pool3 = MaxPooling2D(pool_size = (2, 2))(conv3)\n",
    "        pool3 = Dropout(dropout)(pool3)\n",
    "        # level 4\n",
    "        conv4 = self.__Conv2DBlock__(inputTensor = pool3, filters = 512, kernelSize = 3, batchNorm = batchNorm)\n",
    "        pool4 = MaxPooling2D(pool_size = (2, 2))(conv4)\n",
    "        pool4 = Dropout(dropout)(pool4)\n",
    "        # feature level -- last level\n",
    "        conv5 = self.__Conv2DBlock__(inputTensor = pool4, filters = 1024, kernelSize = 3, batchNorm = batchNorm)\n",
    "        \n",
    "        # Decoder\n",
    "        # level 4\n",
    "        up1 = Conv2DTranspose(filters = 512, kernel_size = (3, 3), strides = (2, 2), padding = \"same\")(conv5)\n",
    "        up1 = concatenate([up1, conv4])\n",
    "        up1 = Dropout(dropout)(up1)\n",
    "        conv6 = self.__Conv2DBlock__(inputTensor = up1, filters = 512, kernelSize = 3, batchNorm = batchNorm)\n",
    "        # level 3\n",
    "        up2 = Conv2DTranspose(filters = 256, kernel_size = (3, 3), strides = (2, 2), padding = \"same\")(conv6)\n",
    "        up2 = concatenate([up2, conv3])\n",
    "        up2 = Dropout(dropout)(up2)\n",
    "        conv7 = self.__Conv2DBlock__(inputTensor = up2, filters = 256, kernelSize = 3, batchNorm = batchNorm)\n",
    "        # level 2\n",
    "        up3 = Conv2DTranspose(filters = 128, kernel_size = (3, 3), strides = (2, 2), padding = \"same\")(conv7)\n",
    "        up3 = concatenate([up3, conv2])\n",
    "        up3 = Dropout(dropout)(up3)\n",
    "        conv8 = self.__Conv2DBlock__(inputTensor = up3, filters = 128, kernelSize = 3, batchNorm = batchNorm)\n",
    "        # level 1\n",
    "        up4 = Conv2DTranspose(filters = 64, kernel_size = (3, 3), strides = (2, 2), padding = \"same\")(conv8)\n",
    "        up4 = concatenate([up4, conv1])\n",
    "        up4 = Dropout(dropout)(up4)\n",
    "        conv9 = self.__Conv2DBlock__(inputTensor = up4, filters = 64, kernelSize = 3, batchNorm = batchNorm)\n",
    "\n",
    "        # output\n",
    "        outputs = Conv2D(filters = n_out, kernel_size = (1, 1), activation = 'sigmoid')(conv9)\n",
    "        \n",
    "        # Make model\n",
    "        model = Model(inputs = [inputImage], outputs = [outputs])\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    # NOTE: Exposed API for usage\n",
    "    def SetHyperParameters(self, learning_rate = 0.001, loss = \"binary_crossentropy\", metrics = [\"accuracy\"], batchSize = 5):\n",
    "        '''\n",
    "        Function to set the hyperparameters\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            learning_rate {float} -- rate at which training occurs (default: {0.001})\n",
    "            loss {str} -- loss function (default: {\"binary_crossentropy\"})\n",
    "            metrics {list} -- metrics to eval (default: {[\"accuracy\"]})\n",
    "            batchSize {int} -- size of the batch (default: {5})\n",
    "        '''\n",
    "        self.learningRate = learning_rate\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        self.batchSize = batchSize\n",
    "        print(\"[INFO] Hyperparams set\")\n",
    "        return True\n",
    "    \n",
    "    def SetupCallbacks(self, checkpoint_dir = \"./checkpoint\", verbose = 1):\n",
    "        '''\n",
    "        Function to setup callbacks\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            checkpoint_dir {str} -- [description] (default: {\"./checkpoint\"})\n",
    "            verbose {int} -- [description] (default: {1})\n",
    "        '''\n",
    "        try:\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                raise FileNotFoundError\n",
    "            elif os.listdir(checkpoint_dir + \"/\") != []:\n",
    "                raise FileExistsError\n",
    "            else:\n",
    "                True\n",
    "        except FileNotFoundError:\n",
    "            print(\"[WARN] Warning: Folder ./checkpoint/ don't exist hence making it\")\n",
    "            log.logger.warning(\"[WARN] Warning: Folder ./checkpoint/ don't exist hence making it\")\n",
    "            os.mkdir(checkpoint_dir)\n",
    "        finally:\n",
    "            self.callbacks = [\n",
    "                EarlyStopping(patience = 10, verbose = verbose),\n",
    "                ReduceLROnPlateau(factor = 0.1, patience = 3, min_lr = 0.000001, verbose = verbose),\n",
    "                ModelCheckpoint(filepath = checkpoint_dir + \"/checkpoint_model.h5\", verbose = verbose, save_best_only = True, save_weights_only = True)\n",
    "            ]\n",
    "            print(\"[INFO] Callbacks set\")\n",
    "            return True\n",
    "    \n",
    "    def CompileModel(self, optimizer = \"Adam\") -> bool:\n",
    "        '''\n",
    "        Function to compile the built model\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            optimizer {str} -- the optimizer for the compilation (default: {\"Adam\"})\n",
    "        \n",
    "        Returns:\n",
    "            bool -- Status check\n",
    "        '''\n",
    "        try:\n",
    "            if not self.model:\n",
    "                raise ModelFrameError\n",
    "            if optimizer == \"Adam\":\n",
    "                self.model.compile(\n",
    "                    optimizer = Adam(lr = self.learningRate),\n",
    "                    loss = self.loss,\n",
    "                    metrics = self.metrics\n",
    "                )\n",
    "            else:\n",
    "                self.model.compile(\n",
    "                    optimizer = optimizer,\n",
    "                    loss = self.loss,\n",
    "                    metrics = self.metrics\n",
    "                )\n",
    "        except ModelFrameError:\n",
    "            print(\"[WARN]: Build Model first!\")\n",
    "            log.logger.warning(\"[WARN]: Build Model first!\")\n",
    "            return False\n",
    "        \n",
    "        except Exception as err:\n",
    "            print(\"[FATAL]: Compilation process failed with unknown exception: \", err)\n",
    "            traceback.print_exc()\n",
    "            log.logger.critical(\"[FATAL]: Compilation process failed with unknown exception: \" + err)\n",
    "            return False\n",
    "\n",
    "        finally:\n",
    "            return True\n",
    "        \n",
    "    def Train(self, XTrain, yTrain, XDev = None, yDev = None, n_epoch = 120):\n",
    "        '''\n",
    "        Function to train the data\n",
    "        \n",
    "        Arguments:\n",
    "            XTrain {numpy array} -- input images for train 240 x 240 x 4\n",
    "            yTrain {numpy array} -- label output of the image for train 240 x 240 x 1\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            XDev {[type]} -- input images for the validation 240 x 240 x 4 (default: {None})\n",
    "            yDev {[type]} -- label output of the image for validation 240 x 240 x 1 (default: {None})\n",
    "            n_epoch {int} -- the total number of the epoch (default: {120})\n",
    "        '''\n",
    "        # Set the data\n",
    "        trainGen = DataGenerator(XSet = XTrain, ySet = yTrain, batch_size = self.batchSize)\n",
    "        if XDev is not None and yDev is not None:\n",
    "            devGen = DataGenerator(XSet = XDev, ySet = yDev, batch_size = self.batchSize)\n",
    "        print(\"[INFO] Training started\")\n",
    "        log.logger.info(\"[INFO] Training started\")\n",
    "\n",
    "        # Train the model\n",
    "        results = self.model.fit_generator(\n",
    "            trainGen,\n",
    "            steps_per_epoch = XTrain.shape[0] // self.batchSize,\n",
    "            epochs = n_epoch,\n",
    "            use_multiprocessing = True,\n",
    "            shuffle = True,\n",
    "            validation_data = devGen,\n",
    "            validation_steps = XDev.shape[0] // self.batchSize,\n",
    "            workers = 0)\n",
    "        log.logger.info(\"[INFO] Train completed\")\n",
    "        # Plot and viz\n",
    "        self.Plot(results = results)\n",
    "           \n",
    "    def Evaluate(self, XTest, yTest):\n",
    "        '''\n",
    "        Function to evaluate the model with test data\n",
    "        \n",
    "        Arguments:\n",
    "            XTest {numpy array} -- X test data for testing\n",
    "            yTest {numpy array} -- y label test data for testing\n",
    "        '''\n",
    "\n",
    "        testGen = DataGenerator(XTest, yTest, self.batchSize)\n",
    "        self.model.evaluate_generator(\n",
    "            testGen,\n",
    "            steps = self.batchSize,\n",
    "            verbose = 1,\n",
    "            use_multiprocessing = True,\n",
    "            workers = 0)\n",
    "    \n",
    "    def Predict(self, XTest, yTest):\n",
    "        for ind, (x, y) in enumerate(zip(XTest, yTest)):\n",
    "            pred = self.model.predict(x)\n",
    "            VisualizeImageWithPrediction(x, y, pred, path = \"./test/test_{}.png\".format(ind))\n",
    "        return True\n",
    "\n",
    "    def Plot(self, results):\n",
    "        plt.figure(figsize = (8, 8))\n",
    "        plt.title(\"Learning Curve\")\n",
    "        plt.plot(results.history[\"loss\"], label = \"loss\")\n",
    "        plt.plot(results.history[\"val_loss\"], label = \"val_loss\")\n",
    "        plt.plot(np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker = \"x\", color = \"r\", label = \"best model\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"log_loss\")\n",
    "        plt.legend()\n",
    "\n",
    "    def SaveModel(self, path = \"./savedModels/\", modelName = \"UNet_Standard\"):\n",
    "        '''\n",
    "        Function to save the model in the given directory\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            path {str} -- path to save the model (default: {\"./savedModels/\"})\n",
    "            modelName {str} -- the name of the model (default: {\"UNet_Standard\"})\n",
    "        '''\n",
    "        log.logger.info(\"[INFO] Saving model as JSON and h5\")\n",
    "        SaveModelJSON(self.model, modelName, path)\n",
    "    \n",
    "    def LoadModelWeights(self, path = \"./checkpoint/checkpoint_model.h5\"):\n",
    "        '''\n",
    "        Function to load the weights alone given in .h5 file in the path\n",
    "\n",
    "        Keyword Arguments:\n",
    "            path {str} -- path for the weights (default: {\"./checkpoint/checkpoint_model.h5\"})\n",
    "        '''\n",
    "        self.model.load_weights(path)\n",
    "        print(\"[INFO] Weight loaded\")\n",
    "        log.logger.info(\"[INFO] Weight loaded\")\n",
    "        return True\n",
    "    \n",
    "    def LoadModel(self, path = \"./savedModels/\", modelName = \"UNet_Standard\"):\n",
    "        '''\n",
    "        Function to load the model\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            path {str} -- the path for the saved model (default: {\"./savedModels/\"})\n",
    "            modelName {str} -- the name of the model (default: {\"UNet_Standard\"})\n",
    "        '''\n",
    "\n",
    "        self.model = LoadModelJSON(modelName, path)\n",
    "        print(\"[INFO] Model Loaded\")\n",
    "        log.logger.info(\"[INFO] Model loaded\")\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "28\n",
      "28\n",
      "(1, 240, 240, 153)\n",
      "(1, 240, 240, 153)\n",
      "(1, 240, 240, 153)\n",
      "(1, 240, 240, 153)\n",
      "{'DWI': {'mean': 23.145506, 'std': 46.76815}, 'Flair': {'mean': 28.769176, 'std': 57.13965}, 'T1': {'mean': 32.706516, 'std': 61.723255}, 'T2': {'mean': 51.48904, 'std': 101.01554}}\n",
      " HGG Validation\n",
      "finished 22\n",
      "finished 23\n",
      "finished 24\n",
      " HGG Train\n",
      "finished 1\n",
      "finished 2\n",
      "finished 3\n",
      "finished 4\n",
      "finished 5\n",
      "finished 6\n",
      "finished 7\n",
      "finished 8\n",
      "finished 9\n",
      "finished 10\n",
      "finished 11\n",
      "finished 12\n",
      "finished 13\n",
      "finished 14\n",
      "finished 15\n",
      "finished 16\n",
      "finished 17\n",
      "finished 18\n",
      "finished 19\n",
      "finished 20\n",
      "finished 21\n",
      "(3213, 240, 240, 4)\n",
      "(3213, 240, 240)\n",
      "Preparing Testing Data\n",
      "finished 25\n",
      "finished 26\n",
      "finished 27\n",
      "finished 28\n",
      "(612, 240, 240, 4)\n",
      "(612, 240, 240)\n"
     ]
    }
   ],
   "source": [
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Data Loaded\n"
     ]
    }
   ],
   "source": [
    "XTrain = dataset.X_train_input\n",
    "yTrain = dataset.X_train_target[:, :, :, np.newaxis]\n",
    "XDev = dataset.X_dev_input\n",
    "yDev = dataset.X_dev_target[:, :, :, np.newaxis]\n",
    "XTest = dataset.X_test_input\n",
    "yTest = dataset.X_test_target[:, :, :, np.newaxis]\n",
    "print(\"[INFO] Data Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Data\n",
    "yTrain = (yTrain > 0).astype(int)\n",
    "yDev = (yDev > 0).astype(int)\n",
    "yTest = (yTest > 0).astype(int)\n",
    "X = np.asarray(XTrain[80])\n",
    "y = np.asarray(yTrain[80])\n",
    "nw, nh, nz = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3213, 240, 240, 4)\n",
      "(3213, 240, 240, 1)\n",
      "(459, 240, 240, 4)\n",
      "(459, 240, 240, 1)\n",
      "(612, 240, 240, 4)\n",
      "(612, 240, 240, 1)\n"
     ]
    }
   ],
   "source": [
    "print(XTrain.shape, yTrain.shape, XDev.shape, yDev.shape, XTest.shape, yTest.shape, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter settings\n",
    "batch_size = 5\n",
    "lr = 0.00001\n",
    "n_epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Input: size of image: 240 240 4\n",
      "[INFO] Model built. Compile it!\n",
      "[INFO] Folders initialized\n",
      "[INFO] Hyperparams set\n",
      "[INFO] Callbacks set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = UNet(shape = (batch_size, nw, nh, nz), dropout = 0.05, batchNorm = True)\n",
    "model.SetHyperParameters(learning_rate = lr, batchSize = batch_size, loss = dice_coef_loss, metrics = [dice_coef])\n",
    "model.SetupCallbacks()\n",
    "model.CompileModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training started\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177/642 [=======>......................] - ETA: 2:48:13 - loss: 0.2463 - dice_coef: 0.75 - ETA: 1:28:33 - loss: 0.2365 - dice_coef: 0.76 - ETA: 1:01:59 - loss: 0.2354 - dice_coef: 0.76 - ETA: 48:40 - loss: 0.2348 - dice_coef: 0.7652 - ETA: 40:41 - loss: 0.2347 - dice_coef: 0.76 - ETA: 35:20 - loss: 0.2353 - dice_coef: 0.76 - ETA: 31:32 - loss: 0.2359 - dice_coef: 0.76 - ETA: 28:41 - loss: 0.2368 - dice_coef: 0.76 - ETA: 26:27 - loss: 0.2373 - dice_coef: 0.76 - ETA: 24:40 - loss: 0.2378 - dice_coef: 0.76 - ETA: 23:14 - loss: 0.2378 - dice_coef: 0.76 - ETA: 22:01 - loss: 0.2377 - dice_coef: 0.76 - ETA: 20:58 - loss: 0.2376 - dice_coef: 0.76 - ETA: 20:05 - loss: 0.2372 - dice_coef: 0.76 - ETA: 19:19 - loss: 0.2366 - dice_coef: 0.76 - ETA: 18:38 - loss: 0.2358 - dice_coef: 0.76 - ETA: 18:02 - loss: 0.2350 - dice_coef: 0.76 - ETA: 17:30 - loss: 0.2340 - dice_coef: 0.76 - ETA: 17:01 - loss: 0.2331 - dice_coef: 0.76 - ETA: 16:35 - loss: 0.2321 - dice_coef: 0.76 - ETA: 16:11 - loss: 0.2313 - dice_coef: 0.76 - ETA: 15:49 - loss: 0.2305 - dice_coef: 0.76 - ETA: 15:30 - loss: 0.2296 - dice_coef: 0.77 - ETA: 15:11 - loss: 0.2289 - dice_coef: 0.77 - ETA: 14:55 - loss: 0.2284 - dice_coef: 0.77 - ETA: 14:39 - loss: 0.2280 - dice_coef: 0.77 - ETA: 14:25 - loss: 0.2277 - dice_coef: 0.77 - ETA: 14:11 - loss: 0.2274 - dice_coef: 0.77 - ETA: 13:59 - loss: 0.2274 - dice_coef: 0.77 - ETA: 13:47 - loss: 0.2274 - dice_coef: 0.77 - ETA: 13:35 - loss: 0.2275 - dice_coef: 0.77 - ETA: 13:25 - loss: 0.2276 - dice_coef: 0.77 - ETA: 13:15 - loss: 0.2276 - dice_coef: 0.77 - ETA: 13:06 - loss: 0.2275 - dice_coef: 0.77 - ETA: 12:57 - loss: 0.2269 - dice_coef: 0.77 - ETA: 12:48 - loss: 0.2264 - dice_coef: 0.77 - ETA: 12:40 - loss: 0.2258 - dice_coef: 0.77 - ETA: 12:33 - loss: 0.2253 - dice_coef: 0.77 - ETA: 12:25 - loss: 0.2248 - dice_coef: 0.77 - ETA: 12:18 - loss: 0.2243 - dice_coef: 0.77 - ETA: 12:12 - loss: 0.2239 - dice_coef: 0.77 - ETA: 12:06 - loss: 0.2235 - dice_coef: 0.77 - ETA: 12:00 - loss: 0.2231 - dice_coef: 0.77 - ETA: 11:54 - loss: 0.2227 - dice_coef: 0.77 - ETA: 11:48 - loss: 0.2223 - dice_coef: 0.77 - ETA: 11:43 - loss: 0.2219 - dice_coef: 0.77 - ETA: 11:38 - loss: 0.2215 - dice_coef: 0.77 - ETA: 11:33 - loss: 0.2210 - dice_coef: 0.77 - ETA: 11:28 - loss: 0.2205 - dice_coef: 0.77 - ETA: 11:23 - loss: 0.2200 - dice_coef: 0.78 - ETA: 11:19 - loss: 0.2195 - dice_coef: 0.78 - ETA: 11:14 - loss: 0.2191 - dice_coef: 0.78 - ETA: 11:10 - loss: 0.2187 - dice_coef: 0.78 - ETA: 11:06 - loss: 0.2183 - dice_coef: 0.78 - ETA: 11:02 - loss: 0.2179 - dice_coef: 0.78 - ETA: 10:58 - loss: 0.2175 - dice_coef: 0.78 - ETA: 10:54 - loss: 0.2171 - dice_coef: 0.78 - ETA: 10:51 - loss: 0.2168 - dice_coef: 0.78 - ETA: 10:47 - loss: 0.2164 - dice_coef: 0.78 - ETA: 10:43 - loss: 0.2161 - dice_coef: 0.78 - ETA: 10:40 - loss: 0.2158 - dice_coef: 0.78 - ETA: 10:37 - loss: 0.2155 - dice_coef: 0.78 - ETA: 10:33 - loss: 0.2152 - dice_coef: 0.78 - ETA: 10:30 - loss: 0.2150 - dice_coef: 0.78 - ETA: 10:27 - loss: 0.2146 - dice_coef: 0.78 - ETA: 10:24 - loss: 0.2142 - dice_coef: 0.78 - ETA: 10:21 - loss: 0.2138 - dice_coef: 0.78 - ETA: 10:18 - loss: 0.2133 - dice_coef: 0.78 - ETA: 10:15 - loss: 0.2129 - dice_coef: 0.78 - ETA: 10:13 - loss: 0.2125 - dice_coef: 0.78 - ETA: 10:10 - loss: 0.2121 - dice_coef: 0.78 - ETA: 10:07 - loss: 0.2117 - dice_coef: 0.78 - ETA: 10:05 - loss: 0.2113 - dice_coef: 0.78 - ETA: 10:02 - loss: 0.2109 - dice_coef: 0.78 - ETA: 9:59 - loss: 0.2105 - dice_coef: 0.7895 - ETA: 9:57 - loss: 0.2101 - dice_coef: 0.789 - ETA: 9:54 - loss: 0.2097 - dice_coef: 0.790 - ETA: 9:52 - loss: 0.2093 - dice_coef: 0.790 - ETA: 9:50 - loss: 0.2089 - dice_coef: 0.791 - ETA: 9:48 - loss: 0.2084 - dice_coef: 0.791 - ETA: 9:45 - loss: 0.2080 - dice_coef: 0.792 - ETA: 9:43 - loss: 0.2076 - dice_coef: 0.792 - ETA: 9:41 - loss: 0.2072 - dice_coef: 0.792 - ETA: 9:38 - loss: 0.2068 - dice_coef: 0.793 - ETA: 9:37 - loss: 0.2065 - dice_coef: 0.793 - ETA: 9:34 - loss: 0.2061 - dice_coef: 0.793 - ETA: 9:32 - loss: 0.2058 - dice_coef: 0.794 - ETA: 9:30 - loss: 0.2055 - dice_coef: 0.794 - ETA: 9:28 - loss: 0.2053 - dice_coef: 0.794 - ETA: 9:26 - loss: 0.2050 - dice_coef: 0.795 - ETA: 9:24 - loss: 0.2047 - dice_coef: 0.795 - ETA: 9:22 - loss: 0.2043 - dice_coef: 0.795 - ETA: 9:20 - loss: 0.2040 - dice_coef: 0.796 - ETA: 9:19 - loss: 0.2038 - dice_coef: 0.796 - ETA: 9:17 - loss: 0.2035 - dice_coef: 0.796 - ETA: 9:15 - loss: 0.2032 - dice_coef: 0.796 - ETA: 9:13 - loss: 0.2028 - dice_coef: 0.797 - ETA: 9:11 - loss: 0.2024 - dice_coef: 0.797 - ETA: 9:10 - loss: 0.2020 - dice_coef: 0.798 - ETA: 9:08 - loss: 0.2015 - dice_coef: 0.798 - ETA: 9:06 - loss: 0.2011 - dice_coef: 0.798 - ETA: 9:04 - loss: 0.2006 - dice_coef: 0.799 - ETA: 9:03 - loss: 0.2002 - dice_coef: 0.799 - ETA: 9:01 - loss: 0.1997 - dice_coef: 0.800 - ETA: 8:59 - loss: 0.1993 - dice_coef: 0.800 - ETA: 8:58 - loss: 0.1988 - dice_coef: 0.801 - ETA: 8:56 - loss: 0.1983 - dice_coef: 0.801 - ETA: 8:54 - loss: 0.1978 - dice_coef: 0.802 - ETA: 8:53 - loss: 0.1973 - dice_coef: 0.802 - ETA: 8:51 - loss: 0.1968 - dice_coef: 0.803 - ETA: 8:50 - loss: 0.1964 - dice_coef: 0.803 - ETA: 8:48 - loss: 0.1958 - dice_coef: 0.804 - ETA: 8:47 - loss: 0.1953 - dice_coef: 0.804 - ETA: 8:45 - loss: 0.1948 - dice_coef: 0.805 - ETA: 8:43 - loss: 0.1943 - dice_coef: 0.805 - ETA: 8:42 - loss: 0.1939 - dice_coef: 0.806 - ETA: 8:40 - loss: 0.1935 - dice_coef: 0.806 - ETA: 8:39 - loss: 0.1931 - dice_coef: 0.806 - ETA: 8:37 - loss: 0.1928 - dice_coef: 0.807 - ETA: 8:36 - loss: 0.1925 - dice_coef: 0.807 - ETA: 8:34 - loss: 0.1922 - dice_coef: 0.807 - ETA: 8:33 - loss: 0.1920 - dice_coef: 0.808 - ETA: 8:32 - loss: 0.1917 - dice_coef: 0.808 - ETA: 8:30 - loss: 0.1914 - dice_coef: 0.808 - ETA: 8:29 - loss: 0.1914 - dice_coef: 0.808 - ETA: 8:27 - loss: 0.1912 - dice_coef: 0.808 - ETA: 8:26 - loss: 0.1910 - dice_coef: 0.809 - ETA: 8:24 - loss: 0.1908 - dice_coef: 0.809 - ETA: 8:23 - loss: 0.1905 - dice_coef: 0.809 - ETA: 8:21 - loss: 0.1901 - dice_coef: 0.809 - ETA: 8:20 - loss: 0.1898 - dice_coef: 0.810 - ETA: 8:19 - loss: 0.1895 - dice_coef: 0.810 - ETA: 8:17 - loss: 0.1891 - dice_coef: 0.810 - ETA: 8:16 - loss: 0.1887 - dice_coef: 0.811 - ETA: 8:14 - loss: 0.1883 - dice_coef: 0.811 - ETA: 8:13 - loss: 0.1879 - dice_coef: 0.812 - ETA: 8:12 - loss: 0.1874 - dice_coef: 0.812 - ETA: 8:10 - loss: 0.1870 - dice_coef: 0.813 - ETA: 8:09 - loss: 0.1866 - dice_coef: 0.813 - ETA: 8:08 - loss: 0.1861 - dice_coef: 0.813 - ETA: 8:07 - loss: 0.1857 - dice_coef: 0.814 - ETA: 8:05 - loss: 0.1853 - dice_coef: 0.814 - ETA: 8:04 - loss: 0.1849 - dice_coef: 0.815 - ETA: 8:03 - loss: 0.1845 - dice_coef: 0.815 - ETA: 8:01 - loss: 0.1841 - dice_coef: 0.815 - ETA: 8:00 - loss: 0.1838 - dice_coef: 0.816 - ETA: 7:59 - loss: 0.1835 - dice_coef: 0.816 - ETA: 7:57 - loss: 0.1833 - dice_coef: 0.816 - ETA: 7:56 - loss: 0.1830 - dice_coef: 0.817 - ETA: 7:55 - loss: 0.1828 - dice_coef: 0.817 - ETA: 7:54 - loss: 0.1827 - dice_coef: 0.817 - ETA: 7:52 - loss: 0.1825 - dice_coef: 0.817 - ETA: 7:51 - loss: 0.1822 - dice_coef: 0.817 - ETA: 7:50 - loss: 0.1819 - dice_coef: 0.818 - ETA: 7:49 - loss: 0.1817 - dice_coef: 0.818 - ETA: 7:47 - loss: 0.1814 - dice_coef: 0.818 - ETA: 7:46 - loss: 0.1814 - dice_coef: 0.818 - ETA: 7:45 - loss: 0.1812 - dice_coef: 0.818 - ETA: 7:44 - loss: 0.1811 - dice_coef: 0.818 - ETA: 7:42 - loss: 0.1808 - dice_coef: 0.819 - ETA: 7:41 - loss: 0.1806 - dice_coef: 0.819 - ETA: 7:40 - loss: 0.1803 - dice_coef: 0.819 - ETA: 7:39 - loss: 0.1799 - dice_coef: 0.820 - ETA: 7:37 - loss: 0.1796 - dice_coef: 0.820 - ETA: 7:36 - loss: 0.1793 - dice_coef: 0.820 - ETA: 7:35 - loss: 0.1789 - dice_coef: 0.821 - ETA: 7:34 - loss: 0.1785 - dice_coef: 0.821 - ETA: 7:33 - loss: 0.1780 - dice_coef: 0.822 - ETA: 7:31 - loss: 0.1776 - dice_coef: 0.822 - ETA: 7:30 - loss: 0.1772 - dice_coef: 0.822 - ETA: 7:29 - loss: 0.1767 - dice_coef: 0.823 - ETA: 7:28 - loss: 0.1763 - dice_coef: 0.823 - ETA: 7:27 - loss: 0.1759 - dice_coef: 0.824 - ETA: 7:26 - loss: 0.1756 - dice_coef: 0.824 - ETA: 7:24 - loss: 0.1752 - dice_coef: 0.824 - ETA: 7:23 - loss: 0.1748 - dice_coef: 0.825 - ETA: 7:22 - loss: 0.1745 - dice_coef: 0.8255\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "355/642 [===============>..............] - ETA: 7:21 - loss: 0.1743 - dice_coef: 0.825 - ETA: 7:20 - loss: 0.1741 - dice_coef: 0.825 - ETA: 7:19 - loss: 0.1739 - dice_coef: 0.826 - ETA: 7:18 - loss: 0.1738 - dice_coef: 0.826 - ETA: 7:16 - loss: 0.1736 - dice_coef: 0.826 - ETA: 7:15 - loss: 0.1733 - dice_coef: 0.826 - ETA: 7:14 - loss: 0.1731 - dice_coef: 0.826 - ETA: 7:13 - loss: 0.1728 - dice_coef: 0.827 - ETA: 7:12 - loss: 0.1726 - dice_coef: 0.827 - ETA: 7:11 - loss: 0.1725 - dice_coef: 0.827 - ETA: 7:10 - loss: 0.1723 - dice_coef: 0.827 - ETA: 7:08 - loss: 0.1722 - dice_coef: 0.827 - ETA: 7:07 - loss: 0.1720 - dice_coef: 0.828 - ETA: 7:06 - loss: 0.1717 - dice_coef: 0.828 - ETA: 7:05 - loss: 0.1714 - dice_coef: 0.828 - ETA: 7:04 - loss: 0.1712 - dice_coef: 0.828 - ETA: 7:03 - loss: 0.1708 - dice_coef: 0.829 - ETA: 7:02 - loss: 0.1705 - dice_coef: 0.829 - ETA: 7:01 - loss: 0.1703 - dice_coef: 0.829 - ETA: 7:00 - loss: 0.1700 - dice_coef: 0.830 - ETA: 6:58 - loss: 0.1696 - dice_coef: 0.830 - ETA: 6:57 - loss: 0.1693 - dice_coef: 0.830 - ETA: 6:56 - loss: 0.1689 - dice_coef: 0.831 - ETA: 6:55 - loss: 0.1686 - dice_coef: 0.831 - ETA: 6:54 - loss: 0.1682 - dice_coef: 0.831 - ETA: 6:53 - loss: 0.1679 - dice_coef: 0.832 - ETA: 6:52 - loss: 0.1675 - dice_coef: 0.832 - ETA: 6:51 - loss: 0.1672 - dice_coef: 0.832 - ETA: 6:50 - loss: 0.1669 - dice_coef: 0.833 - ETA: 6:49 - loss: 0.1666 - dice_coef: 0.833 - ETA: 6:47 - loss: 0.1663 - dice_coef: 0.833 - ETA: 6:46 - loss: 0.1660 - dice_coef: 0.834 - ETA: 6:45 - loss: 0.1658 - dice_coef: 0.834 - ETA: 6:44 - loss: 0.1656 - dice_coef: 0.834 - ETA: 6:43 - loss: 0.1654 - dice_coef: 0.834 - ETA: 6:42 - loss: 0.1653 - dice_coef: 0.834 - ETA: 6:41 - loss: 0.1651 - dice_coef: 0.834 - ETA: 6:40 - loss: 0.1648 - dice_coef: 0.835 - ETA: 6:39 - loss: 0.1647 - dice_coef: 0.835 - ETA: 6:38 - loss: 0.1646 - dice_coef: 0.835 - ETA: 6:37 - loss: 0.1644 - dice_coef: 0.835 - ETA: 6:36 - loss: 0.1642 - dice_coef: 0.835 - ETA: 6:35 - loss: 0.1640 - dice_coef: 0.836 - ETA: 6:34 - loss: 0.1638 - dice_coef: 0.836 - ETA: 6:33 - loss: 0.1635 - dice_coef: 0.836 - ETA: 6:32 - loss: 0.1632 - dice_coef: 0.836 - ETA: 6:30 - loss: 0.1629 - dice_coef: 0.837 - ETA: 6:29 - loss: 0.1626 - dice_coef: 0.837 - ETA: 6:28 - loss: 0.1623 - dice_coef: 0.837 - ETA: 6:27 - loss: 0.1620 - dice_coef: 0.838 - ETA: 6:26 - loss: 0.1617 - dice_coef: 0.838 - ETA: 6:25 - loss: 0.1614 - dice_coef: 0.838 - ETA: 6:24 - loss: 0.1611 - dice_coef: 0.838 - ETA: 6:23 - loss: 0.1608 - dice_coef: 0.839 - ETA: 6:22 - loss: 0.1605 - dice_coef: 0.839 - ETA: 6:21 - loss: 0.1602 - dice_coef: 0.839 - ETA: 6:20 - loss: 0.1599 - dice_coef: 0.840 - ETA: 6:19 - loss: 0.1595 - dice_coef: 0.840 - ETA: 6:18 - loss: 0.1593 - dice_coef: 0.840 - ETA: 6:17 - loss: 0.1590 - dice_coef: 0.841 - ETA: 6:16 - loss: 0.1587 - dice_coef: 0.841 - ETA: 6:15 - loss: 0.1585 - dice_coef: 0.841 - ETA: 6:14 - loss: 0.1582 - dice_coef: 0.841 - ETA: 6:13 - loss: 0.1580 - dice_coef: 0.842 - ETA: 6:12 - loss: 0.1577 - dice_coef: 0.842 - ETA: 6:11 - loss: 0.1576 - dice_coef: 0.842 - ETA: 6:10 - loss: 0.1575 - dice_coef: 0.842 - ETA: 6:09 - loss: 0.1572 - dice_coef: 0.842 - ETA: 6:08 - loss: 0.1570 - dice_coef: 0.843 - ETA: 6:07 - loss: 0.1567 - dice_coef: 0.843 - ETA: 6:06 - loss: 0.1565 - dice_coef: 0.843 - ETA: 6:05 - loss: 0.1563 - dice_coef: 0.843 - ETA: 6:04 - loss: 0.1561 - dice_coef: 0.843 - ETA: 6:03 - loss: 0.1558 - dice_coef: 0.844 - ETA: 6:02 - loss: 0.1556 - dice_coef: 0.844 - ETA: 6:01 - loss: 0.1554 - dice_coef: 0.844 - ETA: 6:00 - loss: 0.1551 - dice_coef: 0.844 - ETA: 5:59 - loss: 0.1548 - dice_coef: 0.845 - ETA: 5:58 - loss: 0.1545 - dice_coef: 0.845 - ETA: 5:57 - loss: 0.1542 - dice_coef: 0.845 - ETA: 5:56 - loss: 0.1539 - dice_coef: 0.846 - ETA: 5:55 - loss: 0.1535 - dice_coef: 0.846 - ETA: 5:54 - loss: 0.1532 - dice_coef: 0.846 - ETA: 5:53 - loss: 0.1529 - dice_coef: 0.847 - ETA: 5:52 - loss: 0.1526 - dice_coef: 0.847 - ETA: 5:51 - loss: 0.1523 - dice_coef: 0.847 - ETA: 5:50 - loss: 0.1520 - dice_coef: 0.848 - ETA: 5:49 - loss: 0.1518 - dice_coef: 0.848 - ETA: 5:48 - loss: 0.1515 - dice_coef: 0.848 - ETA: 5:47 - loss: 0.1513 - dice_coef: 0.848 - ETA: 5:46 - loss: 0.1511 - dice_coef: 0.848 - ETA: 5:45 - loss: 0.1509 - dice_coef: 0.849 - ETA: 5:44 - loss: 0.1506 - dice_coef: 0.849 - ETA: 5:43 - loss: 0.1504 - dice_coef: 0.849 - ETA: 5:42 - loss: 0.1502 - dice_coef: 0.849 - ETA: 5:41 - loss: 0.1500 - dice_coef: 0.850 - ETA: 5:40 - loss: 0.1497 - dice_coef: 0.850 - ETA: 5:39 - loss: 0.1495 - dice_coef: 0.850 - ETA: 5:38 - loss: 0.1493 - dice_coef: 0.850 - ETA: 5:37 - loss: 0.1491 - dice_coef: 0.850 - ETA: 5:36 - loss: 0.1489 - dice_coef: 0.851 - ETA: 5:35 - loss: 0.1486 - dice_coef: 0.851 - ETA: 5:34 - loss: 0.1484 - dice_coef: 0.851 - ETA: 5:33 - loss: 0.1482 - dice_coef: 0.851 - ETA: 5:32 - loss: 0.1479 - dice_coef: 0.852 - ETA: 5:31 - loss: 0.1477 - dice_coef: 0.852 - ETA: 5:30 - loss: 0.1475 - dice_coef: 0.852 - ETA: 5:29 - loss: 0.1473 - dice_coef: 0.852 - ETA: 5:28 - loss: 0.1471 - dice_coef: 0.852 - ETA: 5:27 - loss: 0.1469 - dice_coef: 0.853 - ETA: 5:26 - loss: 0.1467 - dice_coef: 0.853 - ETA: 5:25 - loss: 0.1465 - dice_coef: 0.853 - ETA: 5:25 - loss: 0.1463 - dice_coef: 0.853 - ETA: 5:24 - loss: 0.1461 - dice_coef: 0.853 - ETA: 5:23 - loss: 0.1459 - dice_coef: 0.854 - ETA: 5:22 - loss: 0.1457 - dice_coef: 0.854 - ETA: 5:21 - loss: 0.1455 - dice_coef: 0.854 - ETA: 5:20 - loss: 0.1453 - dice_coef: 0.854 - ETA: 5:19 - loss: 0.1451 - dice_coef: 0.854 - ETA: 5:18 - loss: 0.1449 - dice_coef: 0.855 - ETA: 5:17 - loss: 0.1446 - dice_coef: 0.855 - ETA: 5:16 - loss: 0.1444 - dice_coef: 0.855 - ETA: 5:15 - loss: 0.1442 - dice_coef: 0.855 - ETA: 5:14 - loss: 0.1440 - dice_coef: 0.856 - ETA: 5:13 - loss: 0.1438 - dice_coef: 0.856 - ETA: 5:12 - loss: 0.1436 - dice_coef: 0.856 - ETA: 5:11 - loss: 0.1434 - dice_coef: 0.856 - ETA: 5:10 - loss: 0.1431 - dice_coef: 0.856 - ETA: 5:09 - loss: 0.1429 - dice_coef: 0.857 - ETA: 5:08 - loss: 0.1427 - dice_coef: 0.857 - ETA: 5:07 - loss: 0.1425 - dice_coef: 0.857 - ETA: 5:06 - loss: 0.1424 - dice_coef: 0.857 - ETA: 5:05 - loss: 0.1422 - dice_coef: 0.857 - ETA: 5:04 - loss: 0.1420 - dice_coef: 0.858 - ETA: 5:03 - loss: 0.1418 - dice_coef: 0.858 - ETA: 5:02 - loss: 0.1416 - dice_coef: 0.858 - ETA: 5:01 - loss: 0.1414 - dice_coef: 0.858 - ETA: 5:00 - loss: 0.1412 - dice_coef: 0.858 - ETA: 4:59 - loss: 0.1411 - dice_coef: 0.858 - ETA: 4:58 - loss: 0.1408 - dice_coef: 0.859 - ETA: 4:57 - loss: 0.1406 - dice_coef: 0.859 - ETA: 4:57 - loss: 0.1404 - dice_coef: 0.859 - ETA: 4:56 - loss: 0.1402 - dice_coef: 0.859 - ETA: 4:55 - loss: 0.1400 - dice_coef: 0.860 - ETA: 4:54 - loss: 0.1397 - dice_coef: 0.860 - ETA: 4:53 - loss: 0.1395 - dice_coef: 0.860 - ETA: 4:52 - loss: 0.1392 - dice_coef: 0.860 - ETA: 4:51 - loss: 0.1390 - dice_coef: 0.861 - ETA: 4:50 - loss: 0.1387 - dice_coef: 0.861 - ETA: 4:49 - loss: 0.1385 - dice_coef: 0.861 - ETA: 4:48 - loss: 0.1383 - dice_coef: 0.861 - ETA: 4:47 - loss: 0.1381 - dice_coef: 0.861 - ETA: 4:46 - loss: 0.1380 - dice_coef: 0.862 - ETA: 4:45 - loss: 0.1378 - dice_coef: 0.862 - ETA: 4:44 - loss: 0.1376 - dice_coef: 0.862 - ETA: 4:43 - loss: 0.1374 - dice_coef: 0.862 - ETA: 4:42 - loss: 0.1373 - dice_coef: 0.862 - ETA: 4:41 - loss: 0.1371 - dice_coef: 0.862 - ETA: 4:40 - loss: 0.1370 - dice_coef: 0.863 - ETA: 4:39 - loss: 0.1368 - dice_coef: 0.863 - ETA: 4:38 - loss: 0.1366 - dice_coef: 0.863 - ETA: 4:37 - loss: 0.1365 - dice_coef: 0.863 - ETA: 4:37 - loss: 0.1363 - dice_coef: 0.863 - ETA: 4:36 - loss: 0.1361 - dice_coef: 0.863 - ETA: 4:35 - loss: 0.1359 - dice_coef: 0.864 - ETA: 4:34 - loss: 0.1357 - dice_coef: 0.864 - ETA: 4:33 - loss: 0.1355 - dice_coef: 0.864 - ETA: 4:32 - loss: 0.1353 - dice_coef: 0.864 - ETA: 4:31 - loss: 0.1351 - dice_coef: 0.864 - ETA: 4:30 - loss: 0.1349 - dice_coef: 0.865 - ETA: 4:29 - loss: 0.1348 - dice_coef: 0.865 - ETA: 4:28 - loss: 0.1346 - dice_coef: 0.865 - ETA: 4:27 - loss: 0.1343 - dice_coef: 0.865 - ETA: 4:26 - loss: 0.1341 - dice_coef: 0.865 - ETA: 4:25 - loss: 0.1339 - dice_coef: 0.866 - ETA: 4:24 - loss: 0.1337 - dice_coef: 0.866 - ETA: 4:23 - loss: 0.1335 - dice_coef: 0.866 - ETA: 4:22 - loss: 0.1333 - dice_coef: 0.8667"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "533/642 [=======================>......] - ETA: 4:21 - loss: 0.1331 - dice_coef: 0.866 - ETA: 4:20 - loss: 0.1329 - dice_coef: 0.867 - ETA: 4:19 - loss: 0.1328 - dice_coef: 0.867 - ETA: 4:19 - loss: 0.1326 - dice_coef: 0.867 - ETA: 4:18 - loss: 0.1324 - dice_coef: 0.867 - ETA: 4:17 - loss: 0.1322 - dice_coef: 0.867 - ETA: 4:16 - loss: 0.1320 - dice_coef: 0.868 - ETA: 4:15 - loss: 0.1318 - dice_coef: 0.868 - ETA: 4:14 - loss: 0.1317 - dice_coef: 0.868 - ETA: 4:13 - loss: 0.1315 - dice_coef: 0.868 - ETA: 4:12 - loss: 0.1313 - dice_coef: 0.868 - ETA: 4:11 - loss: 0.1311 - dice_coef: 0.868 - ETA: 4:10 - loss: 0.1309 - dice_coef: 0.869 - ETA: 4:09 - loss: 0.1308 - dice_coef: 0.869 - ETA: 4:08 - loss: 0.1306 - dice_coef: 0.869 - ETA: 4:07 - loss: 0.1304 - dice_coef: 0.869 - ETA: 4:06 - loss: 0.1302 - dice_coef: 0.869 - ETA: 4:05 - loss: 0.1300 - dice_coef: 0.870 - ETA: 4:04 - loss: 0.1298 - dice_coef: 0.870 - ETA: 4:03 - loss: 0.1296 - dice_coef: 0.870 - ETA: 4:03 - loss: 0.1295 - dice_coef: 0.870 - ETA: 4:02 - loss: 0.1293 - dice_coef: 0.870 - ETA: 4:01 - loss: 0.1291 - dice_coef: 0.870 - ETA: 4:00 - loss: 0.1289 - dice_coef: 0.871 - ETA: 3:59 - loss: 0.1287 - dice_coef: 0.871 - ETA: 3:58 - loss: 0.1286 - dice_coef: 0.871 - ETA: 3:57 - loss: 0.1284 - dice_coef: 0.871 - ETA: 3:56 - loss: 0.1282 - dice_coef: 0.871 - ETA: 3:55 - loss: 0.1280 - dice_coef: 0.872 - ETA: 3:54 - loss: 0.1278 - dice_coef: 0.872 - ETA: 3:53 - loss: 0.1276 - dice_coef: 0.872 - ETA: 3:52 - loss: 0.1274 - dice_coef: 0.872 - ETA: 3:51 - loss: 0.1273 - dice_coef: 0.872 - ETA: 3:50 - loss: 0.1271 - dice_coef: 0.872 - ETA: 3:49 - loss: 0.1269 - dice_coef: 0.873 - ETA: 3:49 - loss: 0.1267 - dice_coef: 0.873 - ETA: 3:48 - loss: 0.1266 - dice_coef: 0.873 - ETA: 3:47 - loss: 0.1264 - dice_coef: 0.873 - ETA: 3:46 - loss: 0.1263 - dice_coef: 0.873 - ETA: 3:45 - loss: 0.1261 - dice_coef: 0.873 - ETA: 3:44 - loss: 0.1259 - dice_coef: 0.874 - ETA: 3:43 - loss: 0.1258 - dice_coef: 0.874 - ETA: 3:42 - loss: 0.1256 - dice_coef: 0.874 - ETA: 3:41 - loss: 0.1254 - dice_coef: 0.874 - ETA: 3:40 - loss: 0.1253 - dice_coef: 0.874 - ETA: 3:39 - loss: 0.1251 - dice_coef: 0.874 - ETA: 3:38 - loss: 0.1249 - dice_coef: 0.875 - ETA: 3:37 - loss: 0.1247 - dice_coef: 0.875 - ETA: 3:36 - loss: 0.1246 - dice_coef: 0.875 - ETA: 3:35 - loss: 0.1244 - dice_coef: 0.875 - ETA: 3:35 - loss: 0.1242 - dice_coef: 0.875 - ETA: 3:34 - loss: 0.1240 - dice_coef: 0.876 - ETA: 3:33 - loss: 0.1238 - dice_coef: 0.876 - ETA: 3:32 - loss: 0.1237 - dice_coef: 0.876 - ETA: 3:31 - loss: 0.1235 - dice_coef: 0.876 - ETA: 3:30 - loss: 0.1233 - dice_coef: 0.876 - ETA: 3:29 - loss: 0.1231 - dice_coef: 0.876 - ETA: 3:28 - loss: 0.1229 - dice_coef: 0.877 - ETA: 3:27 - loss: 0.1227 - dice_coef: 0.877 - ETA: 3:26 - loss: 0.1225 - dice_coef: 0.877 - ETA: 3:25 - loss: 0.1223 - dice_coef: 0.877 - ETA: 3:24 - loss: 0.1222 - dice_coef: 0.877 - ETA: 3:23 - loss: 0.1220 - dice_coef: 0.878 - ETA: 3:23 - loss: 0.1218 - dice_coef: 0.878 - ETA: 3:22 - loss: 0.1216 - dice_coef: 0.878 - ETA: 3:21 - loss: 0.1214 - dice_coef: 0.878 - ETA: 3:20 - loss: 0.1212 - dice_coef: 0.878 - ETA: 3:19 - loss: 0.1210 - dice_coef: 0.879 - ETA: 3:18 - loss: 0.1209 - dice_coef: 0.879 - ETA: 3:17 - loss: 0.1207 - dice_coef: 0.879 - ETA: 3:16 - loss: 0.1206 - dice_coef: 0.879 - ETA: 3:15 - loss: 0.1204 - dice_coef: 0.879 - ETA: 3:14 - loss: 0.1203 - dice_coef: 0.879 - ETA: 3:13 - loss: 0.1201 - dice_coef: 0.879 - ETA: 3:12 - loss: 0.1200 - dice_coef: 0.880 - ETA: 3:11 - loss: 0.1198 - dice_coef: 0.880 - ETA: 3:11 - loss: 0.1196 - dice_coef: 0.880 - ETA: 3:10 - loss: 0.1195 - dice_coef: 0.880 - ETA: 3:09 - loss: 0.1193 - dice_coef: 0.880 - ETA: 3:08 - loss: 0.1192 - dice_coef: 0.880 - ETA: 3:07 - loss: 0.1190 - dice_coef: 0.881 - ETA: 3:06 - loss: 0.1189 - dice_coef: 0.881 - ETA: 3:05 - loss: 0.1187 - dice_coef: 0.881 - ETA: 3:04 - loss: 0.1186 - dice_coef: 0.881 - ETA: 3:03 - loss: 0.1184 - dice_coef: 0.881 - ETA: 3:02 - loss: 0.1182 - dice_coef: 0.881 - ETA: 3:01 - loss: 0.1181 - dice_coef: 0.881 - ETA: 3:00 - loss: 0.1179 - dice_coef: 0.882 - ETA: 2:59 - loss: 0.1177 - dice_coef: 0.882 - ETA: 2:59 - loss: 0.1175 - dice_coef: 0.882 - ETA: 2:58 - loss: 0.1173 - dice_coef: 0.882 - ETA: 2:57 - loss: 0.1171 - dice_coef: 0.882 - ETA: 2:56 - loss: 0.1170 - dice_coef: 0.883 - ETA: 2:55 - loss: 0.1168 - dice_coef: 0.883 - ETA: 2:54 - loss: 0.1166 - dice_coef: 0.883 - ETA: 2:53 - loss: 0.1165 - dice_coef: 0.883 - ETA: 2:52 - loss: 0.1163 - dice_coef: 0.883 - ETA: 2:51 - loss: 0.1162 - dice_coef: 0.883 - ETA: 2:50 - loss: 0.1160 - dice_coef: 0.884 - ETA: 2:49 - loss: 0.1158 - dice_coef: 0.884 - ETA: 2:48 - loss: 0.1157 - dice_coef: 0.884 - ETA: 2:48 - loss: 0.1156 - dice_coef: 0.884 - ETA: 2:47 - loss: 0.1154 - dice_coef: 0.884 - ETA: 2:46 - loss: 0.1153 - dice_coef: 0.884 - ETA: 2:45 - loss: 0.1151 - dice_coef: 0.884 - ETA: 2:44 - loss: 0.1150 - dice_coef: 0.885 - ETA: 2:43 - loss: 0.1149 - dice_coef: 0.885 - ETA: 2:42 - loss: 0.1147 - dice_coef: 0.885 - ETA: 2:41 - loss: 0.1146 - dice_coef: 0.885 - ETA: 2:40 - loss: 0.1144 - dice_coef: 0.885 - ETA: 2:39 - loss: 0.1143 - dice_coef: 0.885 - ETA: 2:38 - loss: 0.1141 - dice_coef: 0.885 - ETA: 2:37 - loss: 0.1140 - dice_coef: 0.886 - ETA: 2:37 - loss: 0.1138 - dice_coef: 0.886 - ETA: 2:36 - loss: 0.1137 - dice_coef: 0.886 - ETA: 2:35 - loss: 0.1135 - dice_coef: 0.886 - ETA: 2:34 - loss: 0.1134 - dice_coef: 0.886 - ETA: 2:33 - loss: 0.1132 - dice_coef: 0.886 - ETA: 2:32 - loss: 0.1131 - dice_coef: 0.886 - ETA: 2:31 - loss: 0.1129 - dice_coef: 0.887 - ETA: 2:30 - loss: 0.1128 - dice_coef: 0.887 - ETA: 2:29 - loss: 0.1126 - dice_coef: 0.887 - ETA: 2:28 - loss: 0.1125 - dice_coef: 0.887 - ETA: 2:27 - loss: 0.1123 - dice_coef: 0.887 - ETA: 2:26 - loss: 0.1122 - dice_coef: 0.887 - ETA: 2:26 - loss: 0.1120 - dice_coef: 0.888 - ETA: 2:25 - loss: 0.1119 - dice_coef: 0.888 - ETA: 2:24 - loss: 0.1117 - dice_coef: 0.888 - ETA: 2:23 - loss: 0.1116 - dice_coef: 0.888 - ETA: 2:22 - loss: 0.1115 - dice_coef: 0.888 - ETA: 2:21 - loss: 0.1113 - dice_coef: 0.888 - ETA: 2:20 - loss: 0.1112 - dice_coef: 0.888 - ETA: 2:19 - loss: 0.1110 - dice_coef: 0.889 - ETA: 2:18 - loss: 0.1109 - dice_coef: 0.889 - ETA: 2:17 - loss: 0.1108 - dice_coef: 0.889 - ETA: 2:16 - loss: 0.1108 - dice_coef: 0.889 - ETA: 2:15 - loss: 0.1106 - dice_coef: 0.889 - ETA: 2:15 - loss: 0.1105 - dice_coef: 0.889 - ETA: 2:14 - loss: 0.1104 - dice_coef: 0.889 - ETA: 2:13 - loss: 0.1102 - dice_coef: 0.889 - ETA: 2:12 - loss: 0.1101 - dice_coef: 0.889 - ETA: 2:11 - loss: 0.1099 - dice_coef: 0.890 - ETA: 2:10 - loss: 0.1098 - dice_coef: 0.890 - ETA: 2:09 - loss: 0.1097 - dice_coef: 0.890 - ETA: 2:08 - loss: 0.1095 - dice_coef: 0.890 - ETA: 2:07 - loss: 0.1094 - dice_coef: 0.890 - ETA: 2:06 - loss: 0.1092 - dice_coef: 0.890 - ETA: 2:05 - loss: 0.1091 - dice_coef: 0.890 - ETA: 2:05 - loss: 0.1090 - dice_coef: 0.891 - ETA: 2:04 - loss: 0.1088 - dice_coef: 0.891 - ETA: 2:03 - loss: 0.1087 - dice_coef: 0.891 - ETA: 2:02 - loss: 0.1086 - dice_coef: 0.891 - ETA: 2:01 - loss: 0.1084 - dice_coef: 0.891 - ETA: 2:00 - loss: 0.1083 - dice_coef: 0.891 - ETA: 1:59 - loss: 0.1081 - dice_coef: 0.891 - ETA: 1:58 - loss: 0.1080 - dice_coef: 0.892 - ETA: 1:57 - loss: 0.1079 - dice_coef: 0.892 - ETA: 1:56 - loss: 0.1077 - dice_coef: 0.892 - ETA: 1:55 - loss: 0.1076 - dice_coef: 0.892 - ETA: 1:54 - loss: 0.1075 - dice_coef: 0.892 - ETA: 1:54 - loss: 0.1074 - dice_coef: 0.892 - ETA: 1:53 - loss: 0.1072 - dice_coef: 0.892 - ETA: 1:52 - loss: 0.1071 - dice_coef: 0.892 - ETA: 1:51 - loss: 0.1070 - dice_coef: 0.893 - ETA: 1:50 - loss: 0.1069 - dice_coef: 0.893 - ETA: 1:49 - loss: 0.1068 - dice_coef: 0.893 - ETA: 1:48 - loss: 0.1066 - dice_coef: 0.893 - ETA: 1:47 - loss: 0.1065 - dice_coef: 0.893 - ETA: 1:46 - loss: 0.1064 - dice_coef: 0.893 - ETA: 1:45 - loss: 0.1063 - dice_coef: 0.893 - ETA: 1:44 - loss: 0.1061 - dice_coef: 0.893 - ETA: 1:44 - loss: 0.1060 - dice_coef: 0.894 - ETA: 1:43 - loss: 0.1059 - dice_coef: 0.894 - ETA: 1:42 - loss: 0.1058 - dice_coef: 0.894 - ETA: 1:41 - loss: 0.1056 - dice_coef: 0.894 - ETA: 1:40 - loss: 0.1055 - dice_coef: 0.894 - ETA: 1:39 - loss: 0.1054 - dice_coef: 0.894 - ETA: 1:38 - loss: 0.1052 - dice_coef: 0.8948"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "642/642 [==============================] - ETA: 1:37 - loss: 0.1051 - dice_coef: 0.894 - ETA: 1:36 - loss: 0.1050 - dice_coef: 0.895 - ETA: 1:35 - loss: 0.1049 - dice_coef: 0.895 - ETA: 1:34 - loss: 0.1047 - dice_coef: 0.895 - ETA: 1:34 - loss: 0.1046 - dice_coef: 0.895 - ETA: 1:33 - loss: 0.1045 - dice_coef: 0.895 - ETA: 1:32 - loss: 0.1044 - dice_coef: 0.895 - ETA: 1:31 - loss: 0.1042 - dice_coef: 0.895 - ETA: 1:30 - loss: 0.1041 - dice_coef: 0.895 - ETA: 1:29 - loss: 0.1040 - dice_coef: 0.896 - ETA: 1:28 - loss: 0.1039 - dice_coef: 0.896 - ETA: 1:27 - loss: 0.1037 - dice_coef: 0.896 - ETA: 1:26 - loss: 0.1036 - dice_coef: 0.896 - ETA: 1:25 - loss: 0.1035 - dice_coef: 0.896 - ETA: 1:24 - loss: 0.1034 - dice_coef: 0.896 - ETA: 1:24 - loss: 0.1033 - dice_coef: 0.896 - ETA: 1:23 - loss: 0.1032 - dice_coef: 0.896 - ETA: 1:22 - loss: 0.1031 - dice_coef: 0.896 - ETA: 1:21 - loss: 0.1030 - dice_coef: 0.897 - ETA: 1:20 - loss: 0.1028 - dice_coef: 0.897 - ETA: 1:19 - loss: 0.1027 - dice_coef: 0.897 - ETA: 1:18 - loss: 0.1026 - dice_coef: 0.897 - ETA: 1:17 - loss: 0.1025 - dice_coef: 0.897 - ETA: 1:16 - loss: 0.1024 - dice_coef: 0.897 - ETA: 1:15 - loss: 0.1023 - dice_coef: 0.897 - ETA: 1:14 - loss: 0.1022 - dice_coef: 0.897 - ETA: 1:14 - loss: 0.1021 - dice_coef: 0.897 - ETA: 1:13 - loss: 0.1019 - dice_coef: 0.898 - ETA: 1:12 - loss: 0.1018 - dice_coef: 0.898 - ETA: 1:11 - loss: 0.1017 - dice_coef: 0.898 - ETA: 1:10 - loss: 0.1016 - dice_coef: 0.898 - ETA: 1:09 - loss: 0.1014 - dice_coef: 0.898 - ETA: 1:08 - loss: 0.1013 - dice_coef: 0.898 - ETA: 1:07 - loss: 0.1012 - dice_coef: 0.898 - ETA: 1:06 - loss: 0.1011 - dice_coef: 0.898 - ETA: 1:05 - loss: 0.1009 - dice_coef: 0.899 - ETA: 1:05 - loss: 0.1008 - dice_coef: 0.899 - ETA: 1:04 - loss: 0.1007 - dice_coef: 0.899 - ETA: 1:03 - loss: 0.1006 - dice_coef: 0.899 - ETA: 1:02 - loss: 0.1005 - dice_coef: 0.899 - ETA: 1:01 - loss: 0.1003 - dice_coef: 0.899 - ETA: 1:00 - loss: 0.1002 - dice_coef: 0.899 - ETA: 59s - loss: 0.1001 - dice_coef: 0.899 - ETA: 58s - loss: 0.1000 - dice_coef: 0.90 - ETA: 57s - loss: 0.0999 - dice_coef: 0.90 - ETA: 56s - loss: 0.0998 - dice_coef: 0.90 - ETA: 55s - loss: 0.0997 - dice_coef: 0.90 - ETA: 55s - loss: 0.0996 - dice_coef: 0.90 - ETA: 54s - loss: 0.0995 - dice_coef: 0.90 - ETA: 53s - loss: 0.0993 - dice_coef: 0.90 - ETA: 52s - loss: 0.0992 - dice_coef: 0.90 - ETA: 51s - loss: 0.0991 - dice_coef: 0.90 - ETA: 50s - loss: 0.0990 - dice_coef: 0.90 - ETA: 49s - loss: 0.0989 - dice_coef: 0.90 - ETA: 48s - loss: 0.0988 - dice_coef: 0.90 - ETA: 47s - loss: 0.0987 - dice_coef: 0.90 - ETA: 46s - loss: 0.0986 - dice_coef: 0.90 - ETA: 46s - loss: 0.0985 - dice_coef: 0.90 - ETA: 45s - loss: 0.0983 - dice_coef: 0.90 - ETA: 44s - loss: 0.0982 - dice_coef: 0.90 - ETA: 43s - loss: 0.0981 - dice_coef: 0.90 - ETA: 42s - loss: 0.0980 - dice_coef: 0.90 - ETA: 41s - loss: 0.0979 - dice_coef: 0.90 - ETA: 40s - loss: 0.0978 - dice_coef: 0.90 - ETA: 39s - loss: 0.0976 - dice_coef: 0.90 - ETA: 38s - loss: 0.0975 - dice_coef: 0.90 - ETA: 37s - loss: 0.0974 - dice_coef: 0.90 - ETA: 36s - loss: 0.0973 - dice_coef: 0.90 - ETA: 36s - loss: 0.0972 - dice_coef: 0.90 - ETA: 35s - loss: 0.0971 - dice_coef: 0.90 - ETA: 34s - loss: 0.0970 - dice_coef: 0.90 - ETA: 33s - loss: 0.0969 - dice_coef: 0.90 - ETA: 32s - loss: 0.0967 - dice_coef: 0.90 - ETA: 31s - loss: 0.0966 - dice_coef: 0.90 - ETA: 30s - loss: 0.0965 - dice_coef: 0.90 - ETA: 29s - loss: 0.0964 - dice_coef: 0.90 - ETA: 28s - loss: 0.0963 - dice_coef: 0.90 - ETA: 27s - loss: 0.0962 - dice_coef: 0.90 - ETA: 27s - loss: 0.0961 - dice_coef: 0.90 - ETA: 26s - loss: 0.0961 - dice_coef: 0.90 - ETA: 25s - loss: 0.0960 - dice_coef: 0.90 - ETA: 24s - loss: 0.0959 - dice_coef: 0.90 - ETA: 23s - loss: 0.0958 - dice_coef: 0.90 - ETA: 22s - loss: 0.0957 - dice_coef: 0.90 - ETA: 21s - loss: 0.0956 - dice_coef: 0.90 - ETA: 20s - loss: 0.0955 - dice_coef: 0.90 - ETA: 19s - loss: 0.0954 - dice_coef: 0.90 - ETA: 18s - loss: 0.0952 - dice_coef: 0.90 - ETA: 18s - loss: 0.0951 - dice_coef: 0.90 - ETA: 17s - loss: 0.0950 - dice_coef: 0.90 - ETA: 16s - loss: 0.0949 - dice_coef: 0.90 - ETA: 15s - loss: 0.0948 - dice_coef: 0.90 - ETA: 14s - loss: 0.0947 - dice_coef: 0.90 - ETA: 13s - loss: 0.0946 - dice_coef: 0.90 - ETA: 12s - loss: 0.0945 - dice_coef: 0.90 - ETA: 11s - loss: 0.0944 - dice_coef: 0.90 - ETA: 10s - loss: 0.0943 - dice_coef: 0.90 - ETA: 9s - loss: 0.0942 - dice_coef: 0.9058 - ETA: 9s - loss: 0.0941 - dice_coef: 0.905 - ETA: 8s - loss: 0.0940 - dice_coef: 0.906 - ETA: 7s - loss: 0.0939 - dice_coef: 0.906 - ETA: 6s - loss: 0.0938 - dice_coef: 0.906 - ETA: 5s - loss: 0.0937 - dice_coef: 0.906 - ETA: 4s - loss: 0.0936 - dice_coef: 0.906 - ETA: 3s - loss: 0.0935 - dice_coef: 0.906 - ETA: 2s - loss: 0.0934 - dice_coef: 0.906 - ETA: 1s - loss: 0.0933 - dice_coef: 0.906 - ETA: 0s - loss: 0.0932 - dice_coef: 0.906 - 579s 901ms/step - loss: 0.0931 - dice_coef: 0.9069\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 691200 into shape (5,240,240,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-de1004bda82a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXDev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myDev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-6d1783ebc447>\u001b[0m in \u001b[0;36mTrain\u001b[1;34m(self, XTrain, yTrain, XDev, yDev, n_epoch)\u001b[0m\n\u001b[0;32m    271\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m             \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m             workers = 0)\n\u001b[0m\u001b[0;32m    274\u001b[0m         \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[INFO] Train completed\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;31m# Plot and viz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\A.I\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\A.I\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\A.I\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m                 \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\A.I\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36miter_sequence_infinite\u001b[1;34m(seq)\u001b[0m\n\u001b[0;32m    588\u001b[0m     \"\"\"\n\u001b[0;32m    589\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\A.I\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[1;34m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 372\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    373\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\A.I\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m         \u001b[1;34m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 372\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    373\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Projects\\Brain Tumor Research\\Brain-UNet-Attn\\DataGenerator.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mbLabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mbImages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbImages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mbImages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbImages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbLabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 691200 into shape (5,240,240,4)"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model.Train(XTrain, yTrain, XDev, yDev, n_epoch = n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval model\n",
    "model.Evaluate(XTest, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred model\n",
    "model.Predict(XTest, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.SaveModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model.LoadModel()\n",
    "model.CompileModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.Evaluate(XTest, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
